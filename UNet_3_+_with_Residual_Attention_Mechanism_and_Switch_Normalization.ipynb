{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYARWMJTB8GF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UNet 3 + with Residual Attention Mechanism and Switch Normalization**"
      ],
      "metadata": {
        "id": "4AS26HMECUi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensor\n",
        "import platform\n",
        "import sklearn as sk\n",
        "import sys\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "\n",
        "plt.style.use(\"dark_background\")\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "xKOzP2R0GySn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch Normalization Layer\n",
        "class SwitchNorm2d(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.9, using_moving_average=True, using_bn=True, last_gamma=False):\n",
        "        super(SwitchNorm2d, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.using_moving_average = using_moving_average\n",
        "        self.using_bn = using_bn\n",
        "        self.last_gamma = last_gamma\n",
        "        self.weight = nn.Parameter(torch.ones(1, num_features, 1, 1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1, num_features, 1, 1))\n",
        "        if self.using_bn:\n",
        "            self.mean_weight = nn.Parameter(torch.ones(3))\n",
        "            self.var_weight = nn.Parameter(torch.ones(3))\n",
        "        else:\n",
        "            self.mean_weight = nn.Parameter(torch.ones(2))\n",
        "            self.var_weight = nn.Parameter(torch.ones(2))\n",
        "        if self.using_bn:\n",
        "            self.register_buffer('running_mean', torch.zeros(1, num_features, 1))\n",
        "            self.register_buffer('running_var', torch.zeros(1, num_features, 1))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.using_bn:\n",
        "            self.running_mean.zero_()\n",
        "            self.running_var.zero_()\n",
        "        if self.last_gamma:\n",
        "            self.weight.data.fill_(0)\n",
        "        else:\n",
        "            self.weight.data.fill_(1)\n",
        "        self.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.size()\n",
        "        x = x.view(N, C, -1)\n",
        "        mean_in = x.mean(-1, keepdim=True)\n",
        "        var_in = x.var(-1, keepdim=True)\n",
        "\n",
        "        mean_ln = mean_in.mean(1, keepdim=True)\n",
        "        temp = var_in + mean_in ** 2\n",
        "        var_ln = temp.mean(1, keepdim=True) - mean_ln ** 2\n",
        "\n",
        "        if self.using_bn:\n",
        "            if self.training:\n",
        "                mean_bn = mean_in.mean(0, keepdim=True)\n",
        "                var_bn = temp.mean(0, keepdim=True) - mean_bn ** 2\n",
        "                if self.using_moving_average:\n",
        "                    self.running_mean.mul_(self.momentum)\n",
        "                    self.running_mean.add_((1 - self.momentum) * mean_bn.data)\n",
        "                    self.running_var.mul_(self.momentum)\n",
        "                    self.running_var.add_((1 - self.momentum) * var_bn.data)\n",
        "                else:\n",
        "                    self.running_mean.add_(mean_bn.data)\n",
        "                    self.running_var.add_(mean_bn.data ** 2 + var_bn.data)\n",
        "            else:\n",
        "                mean_bn = torch.autograd.Variable(self.running_mean)\n",
        "                var_bn = torch.autograd.Variable(self.running_var)\n",
        "\n",
        "        softmax = nn.Softmax(0)\n",
        "        mean_weight = softmax(self.mean_weight)\n",
        "        var_weight = softmax(self.var_weight)\n",
        "\n",
        "        if self.using_bn:\n",
        "            mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln + mean_weight[2] * mean_bn\n",
        "            var = var_weight[0] * var_in + var_weight[1] * var_ln + var_weight[2] * var_bn\n",
        "        else:\n",
        "            mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln\n",
        "            var = var_weight[0] * var_in + var_weight[1] * var_ln\n",
        "\n",
        "        x = (x - mean) / (var + self.eps).sqrt()\n",
        "        x = x.view(N, C, H, W)\n",
        "        return x * self.weight + self.bias"
      ],
      "metadata": {
        "id": "EaVssbNcG3OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.sn1 = SwitchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.sn2 = SwitchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.sn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.sn2(out)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "oOvAPOUeG5Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention Block\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, f_g, f_l, f_int):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(f_g, f_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            SwitchNorm2d(f_int)\n",
        "        )\n",
        "\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(f_l, f_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            SwitchNorm2d(f_int)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(f_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            SwitchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        return x * psi"
      ],
      "metadata": {
        "id": "eTYeniwwG7lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNet 3 Plus with Attention and Residual Blocks\n",
        "class UNet3Plus(nn.Module):\n",
        "    def __init__(self, in_channels=3, n_classes=1, feature_scale=4, is_deconv=True, is_batchnorm=True):\n",
        "        super(UNet3Plus, self).__init__()\n",
        "        self.is_deconv = is_deconv\n",
        "        self.in_channels = in_channels\n",
        "        self.is_batchnorm = is_batchnorm\n",
        "        self.feature_scale = feature_scale\n",
        "\n",
        "        filters = [64, 128, 256, 512, 1024]\n",
        "\n",
        "        ## -------------Encoder--------------\n",
        "        self.conv1 = ResidualBlock(self.in_channels, filters[0])\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = ResidualBlock(filters[0], filters[1])\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = ResidualBlock(filters[1], filters[2])\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv4 = ResidualBlock(filters[2], filters[3])\n",
        "        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv5 = ResidualBlock(filters[3], filters[4])\n",
        "\n",
        "        ## -------------Decoder--------------\n",
        "        self.CatChannels = filters[0]\n",
        "        self.CatBlocks = 5\n",
        "        self.UpChannels = self.CatChannels * self.CatBlocks\n",
        "\n",
        "        '''stage 4d'''\n",
        "        self.h1_PT_hd4 = nn.MaxPool2d(8, 8, ceil_mode=True)\n",
        "        self.h1_PT_hd4_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
        "        self.h1_PT_hd4_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.h1_PT_hd4_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.h2_PT_hd4 = nn.MaxPool2d(4, 4, ceil_mode=True)\n",
        "        self.h2_PT_hd4_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
        "        self.h2_PT_hd4_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.h2_PT_hd4_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.h3_PT_hd4 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
        "        self.h3_PT_hd4_conv = nn.Conv2d(filters[2], self.CatChannels, 3, padding=1)\n",
        "        self.h3_PT_hd4_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.h3_PT_hd4_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.h4_Cat_hd4_conv = nn.Conv2d(filters[3], self.CatChannels, 3, padding=1)\n",
        "        self.h4_Cat_hd4_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.h4_Cat_hd4_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.hd5_UT_hd4 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.hd5_UT_hd4_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
        "        self.hd5_UT_hd4_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.hd5_UT_hd4_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.att4 = AttentionBlock(f_g=self.CatChannels, f_l=self.CatChannels, f_int=self.CatChannels // 2)\n",
        "\n",
        "        self.conv4d_1 = nn.Conv2d(384, self.UpChannels, 3, padding=1)\n",
        "        self.bn4d_1 = SwitchNorm2d(self.UpChannels)\n",
        "        self.relu4d_1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        '''stage 3d'''\n",
        "        self.h1_PT_hd3 = nn.MaxPool2d(4, 4, ceil_mode=True)\n",
        "        self.h1_PT_hd3_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
        "        self.h1_PT_hd3_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.h1_PT_hd3_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.h2_PT_hd3 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
        "        self.h2_PT_hd3_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
        "        self.h2_PT_hd3_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.h2_PT_hd3_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.h3_Cat_hd3_conv = nn.Conv2d(filters[2], self.CatChannels, 3, padding=1)\n",
        "        self.h3_Cat_hd3_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.h3_Cat_hd3_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.hd4_UT_hd3 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.hd4_UT_hd3_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
        "        self.hd4_UT_hd3_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.hd4_UT_hd3_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.hd5_UT_hd3 = nn.Upsample(scale_factor=4, mode='bilinear')\n",
        "        self.hd5_UT_hd3_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
        "        self.hd5_UT_hd3_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.hd5_UT_hd3_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.att3 = AttentionBlock(f_g=self.CatChannels, f_l=self.CatChannels, f_int=self.CatChannels // 2)\n",
        "\n",
        "        self.conv3d_1 = nn.Conv2d(384, self.UpChannels, 3, padding=1)\n",
        "        self.bn3d_1 = SwitchNorm2d(self.UpChannels)\n",
        "        self.relu3d_1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        '''stage 2d'''\n",
        "        self.h1_PT_hd2 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
        "        self.h1_PT_hd2_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
        "        self.h1_PT_hd2_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.h1_PT_hd2_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.h2_Cat_hd2_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
        "        self.h2_Cat_hd2_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.h2_Cat_hd2_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.hd3_UT_hd2 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.hd3_UT_hd2_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
        "        self.hd3_UT_hd2_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.hd3_UT_hd2_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.hd4_UT_hd2 = nn.Upsample(scale_factor=4, mode='bilinear')\n",
        "        self.hd4_UT_hd2_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
        "        self.hd4_UT_hd2_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.hd4_UT_hd2_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.hd5_UT_hd2 = nn.Upsample(scale_factor=8, mode='bilinear')\n",
        "        self.hd5_UT_hd2_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
        "        self.hd5_UT_hd2_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.hd5_UT_hd2_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.att2 = AttentionBlock(f_g=self.CatChannels, f_l=self.CatChannels, f_int=self.CatChannels // 2)\n",
        "\n",
        "        self.conv2d_1 = nn.Conv2d(384, self.UpChannels, 3, padding=1)\n",
        "        self.bn2d_1 = SwitchNorm2d(self.UpChannels)\n",
        "        self.relu2d_1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        '''stage 1d'''\n",
        "        self.h1_Cat_hd1_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
        "        self.h1_Cat_hd1_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.h1_Cat_hd1_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.hd2_UT_hd1 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.hd2_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
        "        self.hd2_UT_hd1_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.hd2_UT_hd1_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.hd3_UT_hd1 = nn.Upsample(scale_factor=4, mode='bilinear')\n",
        "        self.hd3_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
        "        self.hd3_UT_hd1_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.hd3_UT_hd1_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.hd4_UT_hd1 = nn.Upsample(scale_factor=8, mode='bilinear')\n",
        "        self.hd4_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
        "        self.hd4_UT_hd1_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.hd4_UT_hd1_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.hd5_UT_hd1 = nn.Upsample(scale_factor=16, mode='bilinear')\n",
        "        self.hd5_UT_hd1_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
        "        self.hd5_UT_hd1_sn = SwitchNorm2d(self.CatChannels)\n",
        "        self.hd5_UT_hd1_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.att1 = AttentionBlock(f_g=self.CatChannels, f_l=self.CatChannels, f_int=self.CatChannels // 2)\n",
        "\n",
        "        self.conv1d_1 = nn.Conv2d(384, self.UpChannels, 3, padding=1)\n",
        "        self.bn1d_1 = SwitchNorm2d(self.UpChannels)\n",
        "        self.relu1d_1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.outconv1 = nn.Conv2d(self.UpChannels, n_classes, 3, padding=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        ## -------------Encoder-------------\n",
        "        h1 = self.conv1(inputs)\n",
        "        h2 = self.maxpool1(h1)\n",
        "        h2 = self.conv2(h2)\n",
        "        h3 = self.maxpool2(h2)\n",
        "        h3 = self.conv3(h3)\n",
        "        h4 = self.maxpool3(h3)\n",
        "        h4 = self.conv4(h4)\n",
        "        h5 = self.maxpool4(h4)\n",
        "        hd5 = self.conv5(h5)\n",
        "\n",
        "        ## -------------Decoder-------------\n",
        "        h1_PT_hd4 = self.h1_PT_hd4_relu(self.h1_PT_hd4_sn(self.h1_PT_hd4_conv(self.h1_PT_hd4(h1))))\n",
        "        h2_PT_hd4 = self.h2_PT_hd4_relu(self.h2_PT_hd4_sn(self.h2_PT_hd4_conv(self.h2_PT_hd4(h2))))\n",
        "        h3_PT_hd4 = self.h3_PT_hd4_relu(self.h3_PT_hd4_sn(self.h3_PT_hd4_conv(self.h3_PT_hd4(h3))))\n",
        "        h4_Cat_hd4 = self.h4_Cat_hd4_relu(self.h4_Cat_hd4_sn(self.h4_Cat_hd4_conv(h4)))\n",
        "        hd5_UT_hd4 = self.hd5_UT_hd4_relu(self.hd5_UT_hd4_sn(self.hd5_UT_hd4_conv(self.hd5_UT_hd4(hd5))))\n",
        "        temp_cat4 = torch.cat((h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4), 1)\n",
        "        outatt4 = self.att4(g=temp_cat4, x=hd5_UT_hd4)\n",
        "        hd4 = self.relu4d_1(self.bn4d_1(self.conv4d_1(torch.cat((h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4, outatt4, hd5_UT_hd4), 1))))\n",
        "\n",
        "        h1_PT_hd3 = self.h1_PT_hd3_relu(self.h1_PT_hd3_sn(self.h1_PT_hd3_conv(self.h1_PT_hd3(h1))))\n",
        "        h2_PT_hd3 = self.h2_PT_hd3_relu(self.h2_PT_hd3_sn(self.h2_PT_hd3_conv(self.h2_PT_hd3(h2))))\n",
        "        h3_Cat_hd3 = self.h3_Cat_hd3_relu(self.h3_Cat_hd3_sn(self.h3_Cat_hd3_conv(h3)))\n",
        "        hd4_UT_hd3 = self.hd4_UT_hd3_relu(self.hd4_UT_hd3_sn(self.hd4_UT_hd3_conv(self.hd4_UT_hd3(hd4))))\n",
        "        hd5_UT_hd3 = self.hd5_UT_hd3_relu(self.hd5_UT_hd3_sn(self.hd5_UT_hd3_conv(self.hd5_UT_hd3(hd5))))\n",
        "        temp_cat3 = torch.cat((h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd5_UT_hd3), 1)\n",
        "        outatt3 = self.att3(g=temp_cat3, x=hd4_UT_hd3)\n",
        "        hd3 = self.relu3d_1(self.bn3d_1(self.conv3d_1(torch.cat((h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, outatt3, hd5_UT_hd3), 1))))\n",
        "\n",
        "        h1_PT_hd2 = self.h1_PT_hd2_relu(self.h1_PT_hd2_sn(self.h1_PT_hd2_conv(self.h1_PT_hd2(h1))))\n",
        "        h2_Cat_hd2 = self.h2_Cat_hd2_relu(self.h2_Cat_hd2_sn(self.h2_Cat_hd2_conv(h2)))\n",
        "        hd3_UT_hd2 = self.hd3_UT_hd2_relu(self.hd3_UT_hd2_sn(self.hd3_UT_hd2_conv(self.hd3_UT_hd2(hd3))))\n",
        "        hd4_UT_hd2 = self.hd4_UT_hd2_relu(self.hd4_UT_hd2_sn(self.hd4_UT_hd2_conv(self.hd4_UT_hd2(hd4))))\n",
        "        hd5_UT_hd2 = self.hd5_UT_hd2_relu(self.hd5_UT_hd2_sn(self.hd5_UT_hd2_conv(self.hd5_UT_hd2(hd5))))\n",
        "        temp_cat2 = torch.cat((h1_PT_hd2, h2_Cat_hd2, hd5_UT_hd2, hd4_UT_hd2), 1)\n",
        "        outatt2 = self.att2(g=temp_cat2, x=hd3_UT_hd2)\n",
        "        hd2 = self.relu2d_1(self.bn2d_1(self.conv2d_1(torch.cat((h1_PT_hd2, h2_Cat_hd2, hd3_UT_hd2, hd4_UT_hd2, outatt2, hd5_UT_hd2), 1))))\n",
        "\n",
        "        h1_Cat_hd1 = self.h1_Cat_hd1_relu(self.h1_Cat_hd1_sn(self.h1_Cat_hd1_conv(h1)))\n",
        "        hd2_UT_hd1 = self.hd2_UT_hd1_relu(self.hd2_UT_hd1_sn(self.hd2_UT_hd1_conv(self.hd2_UT_hd1(hd2))))\n",
        "        hd3_UT_hd1 = self.hd3_UT_hd1_relu(self.hd3_UT_hd1_sn(self.hd3_UT_hd1_conv(self.hd3_UT_hd1(hd3))))\n",
        "        hd4_UT_hd1 = self.hd4_UT_hd1_relu(self.hd4_UT_hd1_sn(self.hd4_UT_hd1_conv(self.hd4_UT_hd1(hd4))))\n",
        "        hd5_UT_hd1 = self.hd5_UT_hd1_relu(self.hd5_UT_hd1_sn(self.hd5_UT_hd1_conv(self.hd5_UT_hd1(hd5))))\n",
        "        temp_cat1 = torch.cat((h1_Cat_hd1, hd5_UT_hd1, hd3_UT_hd1, hd4_UT_hd1), 1)\n",
        "        outatt1 = self.att1(g=temp_cat1, x=hd2_UT_hd1)\n",
        "        hd1 = self.relu1d_1(self.bn1d_1(self.conv1d_1(torch.cat((h1_Cat_hd1, hd2_UT_hd1, hd3_UT_hd1, hd4_UT_hd1, outatt1, hd5_UT_hd1), 1))))\n",
        "\n",
        "        d1 = self.outconv1(hd1)\n",
        "        return F.sigmoid(d1)"
      ],
      "metadata": {
        "id": "sfkrVy9KG__q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dice Metric and Loss Function\n",
        "def dice_coef_metric(inputs, target):\n",
        "    intersection = 2.0 * (target*inputs).sum()\n",
        "    union = target.sum() + inputs.sum()\n",
        "    if target.sum() == 0 and inputs.sum() == 0:\n",
        "        return 1.0\n",
        "    return intersection/union\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "        return 1 - dice\n",
        "\n",
        "def compute_iou(model, loader, device:str, threshold=0.3):\n",
        "    valloss = 0\n",
        "    with torch.no_grad():\n",
        "        for i_step, (data, target) in enumerate(loader):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            outputs = model(data)\n",
        "            out_cut = np.copy(outputs.data.cpu().numpy())\n",
        "            out_cut[np.nonzero(out_cut < threshold)] = 0.0\n",
        "            out_cut[np.nonzero(out_cut >= threshold)] = 1.0\n",
        "            picloss = dice_coef_metric(out_cut, target.data.cpu().numpy())\n",
        "            valloss += picloss\n",
        "    return valloss / i_step"
      ],
      "metadata": {
        "id": "QNriG5enHCQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation Functions\n",
        "def pos_neg_diagnosis(mask_path):\n",
        "    val = np.max(cv2.imread(mask_path))\n",
        "    if val > 0: return 1\n",
        "    else: return 0\n",
        "\n",
        "def get_dataset_dataframe(base_path:str):\n",
        "    data = []\n",
        "    for dir_ in os.listdir(base_path):\n",
        "        dir_path = os.path.join(base_path, dir_)\n",
        "        if os.path.isdir(dir_path):\n",
        "            for filename in os.listdir(dir_path):\n",
        "                img_path = os.path.join(dir_path, filename)\n",
        "                data.append([dir_, img_path])\n",
        "        else:\n",
        "            print(f\"[INFO] This is not a dir --> {dir_path}\")\n",
        "    return pd.DataFrame(data, columns=[\"dir_name\", \"image_path\"])"
      ],
      "metadata": {
        "id": "LbapV8ArHEgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset Class\n",
        "class BrainMRIDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = cv2.imread(self.df.iloc[idx, 1])\n",
        "        mask = cv2.imread(self.df.iloc[idx, 2], 0)\n",
        "\n",
        "        augmented = self.transform(image=image, mask=mask)\n",
        "        image = augmented[\"image\"]\n",
        "        mask = augmented[\"mask\"]\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "fJxpjTw1HG6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation\n",
        "class AugmentData:\n",
        "    def __init__(self):\n",
        "        self.transform = A.Compose([\n",
        "            A.Resize(256, 256),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "augmentData = AugmentData()"
      ],
      "metadata": {
        "id": "T3KR4RUgHJGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show Augmented Images\n",
        "def show_aug(inputs, nrows=5, ncols=5, norm=False):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.subplots_adjust(wspace=0., hspace=0.)\n",
        "    i_ = 0\n",
        "\n",
        "    if len(inputs) > 25:\n",
        "        inputs = inputs[:25]\n",
        "\n",
        "    for idx in range(len(inputs)):\n",
        "        if norm:\n",
        "            img = inputs[idx].numpy().transpose(1,2,0)\n",
        "            mean = [0.485, 0.456, 0.406]\n",
        "            std = [0.229, 0.224, 0.225]\n",
        "            img = (img*std+mean).astype(np.float32)\n",
        "        else:\n",
        "            img = inputs[idx].numpy().astype(np.float32)\n",
        "            img = img[0,:,:]\n",
        "\n",
        "        plt.subplot(nrows, ncols, i_+1)\n",
        "        plt.imshow(img);\n",
        "        plt.axis('off')\n",
        "        i_ += 1\n",
        "\n",
        "    return plt.show()"
      ],
      "metadata": {
        "id": "2xtCO7hXHLJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save and Load Checkpoints\n",
        "def save_ckp(state, is_best, checkpoint_dir, best_model_dir):\n",
        "    f_path = checkpoint_dir + '\\\\checkpoint.pt'\n",
        "    torch.save(state, f_path)\n",
        "    if is_best:\n",
        "        best_fpath = best_model_dir + '\\\\best_model.pt'\n",
        "        shutil.copyfile(f_path, best_fpath)\n",
        "\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print('Previously trained model weights state_dict loaded...')\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    print('Previously trained optimizer state_dict loaded...')\n",
        "    last_epoch = checkpoint['epoch']\n",
        "    print(f\"Previously trained for {last_epoch} number of epochs...\")\n",
        "    return model, optimizer, last_epoch"
      ],
      "metadata": {
        "id": "C7cm66nbHNsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training\n",
        "def train_model(model_name, model, train_loader, val_loader, train_loss, optimizer, lr_scheduler, num_epochs, device, ckp_path:str=None):\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"total params of {model_name} model: {pytorch_total_params}\")\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"trainable params of {model_name} model: {pytorch_total_params}\")\n",
        "\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "    start_epoch=0\n",
        "\n",
        "    if ckp_path is not None:\n",
        "        model, optimizer, last_epoch = load_ckp(ckp_path, model, optimizer)\n",
        "        start_epoch = last_epoch + 1\n",
        "        print(f\"Train for {num_epochs} more epochs...\")\n",
        "\n",
        "    print(f\"[INFO] Model is initializing... {model_name}\")\n",
        "    checkpoint_dir = f\"C:\\\\Users\\\\Ryan\\\\Documents\\\\checkpoints\\\\{model_name}\"\n",
        "    best_model_dir = f\"{checkpoint_dir}\\\\{model_name}_best\"\n",
        "\n",
        "    loss_history = []\n",
        "    train_history = []\n",
        "    val_history = []\n",
        "\n",
        "    mean_loss_ = 999\n",
        "\n",
        "    for epoch in range(start_epoch, start_epoch+num_epochs):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        train_iou = []\n",
        "\n",
        "        for i_step, (data, target) in enumerate(tqdm(train_loader)):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            outputs = model(data)\n",
        "            out_cut = np.copy(outputs.data.cpu().numpy())\n",
        "            out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n",
        "            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n",
        "            train_dice = dice_coef_metric(out_cut, target.data.cpu().numpy())\n",
        "            loss = train_loss(outputs, target)\n",
        "            losses.append(loss.item())\n",
        "            train_iou.append(train_dice)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        val_mean_iou = compute_iou(model, val_loader, device=device)\n",
        "        mean_loss = np.array(losses).mean()\n",
        "        scheduler.step(mean_loss)\n",
        "        loss_history.append(mean_loss)\n",
        "        train_history.append(np.array(train_iou).mean())\n",
        "        val_history.append(val_mean_iou)\n",
        "\n",
        "        checkpoint = {\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': np.array(losses).mean(),\n",
        "                    }\n",
        "        save_ckp(checkpoint, False, checkpoint_dir, best_model_dir)\n",
        "\n",
        "        if loss<mean_loss_:\n",
        "            save_ckp(checkpoint, True, checkpoint_dir, best_model_dir)\n",
        "            mean_loss_ = loss\n",
        "\n",
        "        print(\"Epoch [%d]\" % (epoch))\n",
        "        print(\"Mean loss on train:\", np.array(losses).mean(),\n",
        "              \"\\nMean DICE on train:\", np.array(train_iou).mean(),\n",
        "              \"\\nMean DICE on validation:\", val_mean_iou)\n",
        "\n",
        "    return loss_history, train_history, val_history"
      ],
      "metadata": {
        "id": "7kRVgZgvHQ_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Model History\n",
        "def plot_model_history(model_name, train_history, val_history, num_epochs):\n",
        "    x = np.arange(num_epochs)\n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x, train_history, label='train dice', lw=3, c=\"springgreen\")\n",
        "    plt.plot(x, val_history, label='validation dice', lw=3, c=\"deeppink\")\n",
        "    plt.title(f\"{model_name}\", fontsize=15)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.xlabel(\"Epoch\", fontsize=15)\n",
        "    plt.ylabel(\"DICE\", fontsize=15)\n",
        "    fn = str(int(time.time())) + \".png\"\n",
        "    plt.savefig(f'{model_name}/{model_name}_dice.png', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize Predicted Output\n",
        "def viz_pred_output(model, loader, idx, test_dataset, device=\"mps\", threshold=0.3):\n",
        "    with torch.no_grad():\n",
        "        target = torch.tensor(test_dataset[idx][1])\n",
        "        data = torch.tensor(test_dataset[idx][0])\n",
        "        data = data.to(device).unsqueeze(0)\n",
        "        target = target.to(device).unsqueeze(0)\n",
        "        outputs = model(data)\n",
        "        out_cut = np.copy(outputs.data.cpu().numpy())\n",
        "        out_cut[np.nonzero(out_cut < threshold)] = 0.0\n",
        "        out_cut[np.nonzero(out_cut >= threshold)] = 1.0\n",
        "        f, axarr = plt.subplots(1,2)\n",
        "        targ = target.data.cpu().numpy()[0][0]\n",
        "        target_img = cv2.merge((targ,targ,targ))\n",
        "        axarr[0].imshow(target_img)\n",
        "        op = out_cut[0][0]\n",
        "        axarr[1].imshow(op)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RZHPjE5EB9Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Code Execution\n",
        "data_location = \"path_to_data\"\n",
        "BASE_LEN = len(data_location) + len(\"\\\\TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_\")\n",
        "END_LEN = len(\".tif\")\n",
        "END_MASK_LEN = len(\"_mask.tif\")\n",
        "IMG_SIZE = 512\n",
        "\n",
        "df = get_dataset_dataframe(data_location)\n",
        "df_imgs = df[~df[\"image_path\"].str.contains(\"mask\")]\n",
        "df_masks = df[df[\"image_path\"].str.contains(\"mask\")]\n",
        "\n",
        "imgs = sorted(df_imgs[\"image_path\"].values, key= lambda x: int((x[BASE_LEN: -END_LEN])))\n",
        "masks = sorted(df_masks[\"image_path\"].values, key=lambda x: int((x[BASE_LEN: -END_MASK_LEN])))\n",
        "\n",
        "dff = pd.DataFrame({\"patient\": df_imgs.dir_name.values, \"image_path\": imgs, \"mask_path\": masks})\n",
        "dff[\"diagnosis\"] = dff[\"mask_path\"].apply(lambda x: pos_neg_diagnosis(x))\n",
        "\n",
        "train_df, val_df = train_test_split(dff, stratify=dff.diagnosis, test_size=0.1)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "train_df, test_df = train_test_split(train_df, stratify=train_df.diagnosis, test_size=0.12)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "\n",
        "train_dataset = BrainMRIDataset(train_df, transform=augmentData.transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "val_dataset = BrainMRIDataset(val_df, transform=augmentData.transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=10,  shuffle=True)\n",
        "test_dataset = BrainMRIDataset(test_df, transform=augmentData.transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=10,shuffle=True)\n",
        "\n",
        "images, masks = next(iter(train_dataloader))\n",
        "show_aug(images)\n",
        "show_aug(masks, norm=False)\n",
        "\n",
        "device = \"mps\" if getattr(torch,'has_mps',False) else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = UNet3Plus(in_channels=3, n_classes=1).to(device)\n",
        "criterion = DiceLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "loss_history, train_history, val_history = train_model(\n",
        "    model_name=\"UNet3Plus_Attn_SN\",\n",
        "    model=model,\n",
        "    train_loader=train_dataloader,\n",
        "    val_loader=val_dataloader,\n",
        "    train_loss=criterion,\n",
        "    optimizer=optimizer,\n",
        "    lr_scheduler=None,\n",
        "    num_epochs=25,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "plot_model_history(\"UNet3Plus_Attn_SN\", train_history, val_history, 25)\n",
        "viz_pred_output(model, test_dataloader, idx=0, test_dataset=test_dataset, device=device, threshold=0.3)"
      ],
      "metadata": {
        "id": "izu51ErFHYME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Switch Normalization:** Added a class for SwitchNorm2d and replaced all batch normalization layers with switch normalization layers.\n",
        "\n",
        "**Residual Blocks:** Implemented ResidualBlock class and used it in the UNet encoder and decoder stages.\n",
        "\n",
        "**Attention Blocks:** Integrated AttentionBlock class to add attention mechanisms in the decoder stages.\n",
        "\n",
        "**Data Preparation:** Added functions and classes for dataset preparation and augmentation.\n",
        "\n",
        "**Training and Evaluation:** Included functions for training, evaluating, and visualizing the model.\n",
        "\n",
        "**Note:** Ensure to replace \"path_to_data\" with the actual path to your\n",
        "dataset.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pZmRxE0ECSxH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ton6KpxpE6OJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}