import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
from PIL import Image
import copy
from collections import OrderedDict
import torch.nn.functional as F
import cv2
import matplotlib.pyplot as plt
import albumentations as A
from albumentations.pytorch import ToTensorV2
from tqdm.notebook import tqdm
import shutil
import time

# Custom modules for attention and normalization
class SwitchNorm2d(nn.Module):
    def __init__(self, num_features):
        super(SwitchNorm2d, self).__init__()
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
        self.mean_weight = nn.Parameter(torch.ones(3) / 3)
        self.var_weight = nn.Parameter(torch.ones(3) / 3)
        self.eps = 1e-5

    def forward(self, x):
        mean_in = x.mean(1, keepdim=True)
        var_in = x.var(1, keepdim=True)
        mean_ln = x.mean([1, 2, 3], keepdim=True)
        var_ln = x.var([1, 2, 3], keepdim=True)
        mean_bn = x.mean([0, 2, 3], keepdim=True)
        var_bn = x.var([0, 2, 3], keepdim=True)

        mean = self.mean_weight[0] * mean_in + self.mean_weight[1] * mean_ln + self.mean_weight[2] * mean_bn
        var = self.var_weight[0] * var_in + self.var_weight[1] * var_ln + self.var_weight[2] * var_bn

        x = (x - mean) / (torch.sqrt(var + self.eps))
        return x * self.weight.view(1, -1, 1, 1) + self.bias.view(1, -1, 1, 1)

class AttentionBlock(nn.Module):
    def __init__(self, F_g, F_l, F_int):
        super(AttentionBlock, self).__init__()
        self.W_g = nn.Sequential(
            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),
            nn.BatchNorm2d(F_int)
        )

        self.W_x = nn.Sequential(
            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),
            nn.BatchNorm2d(F_int)
        )

        self.psi = nn.Sequential(
            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )

        self.relu = nn.ReLU(inplace=True)

    def forward(self, g, x):
        g1 = self.W_g(g)
        x1 = self.W_x(x)
        psi = self.relu(g1 + x1)
        psi = self.psi(psi)
        return x * psi

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.sn1 = SwitchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.sn2 = SwitchNorm2d(out_channels)

        if in_channels != out_channels:
            self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        else:
            self.residual_conv = None

    def forward(self, x):
        residual = x
        if self.residual_conv:
            residual = self.residual_conv(x)

        out = self.conv1(x)
        out = self.sn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.sn2(out)
        out += residual
        out = self.relu(out)
        return out

class UNet3Plus(nn.Module):
    def __init__(self, in_channels=3, out_channels=1):
        super(UNet3Plus, self).__init__()
        filters = [64, 128, 256, 512, 1024]

        self.conv1 = ResidualBlock(in_channels, filters[0])
        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv2 = ResidualBlock(filters[0], filters[1])
        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv3 = ResidualBlock(filters[1], filters[2])
        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv4 = ResidualBlock(filters[2], filters[3])
        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv5 = ResidualBlock(filters[3], filters[4])

        self.up5 = nn.ConvTranspose2d(filters[4], filters[3], kernel_size=2, stride=2)
        self.att5 = AttentionBlock(F_g=filters[3], F_l=filters[3], F_int=filters[2])
        self.up_conv5 = ResidualBlock(filters[4], filters[3])

        self.up4 = nn.ConvTranspose2d(filters[3], filters[2], kernel_size=2, stride=2)
        self.att4 = AttentionBlock(F_g=filters[2], F_l=filters[2], F_int=filters[1])
        self.up_conv4 = ResidualBlock(filters[3], filters[2])

        self.up3 = nn.ConvTranspose2d(filters[2], filters[1], kernel_size=2, stride=2)
        self.att3 = AttentionBlock(F_g=filters[1], F_l=filters[1], F_int=filters[0])
        self.up_conv3 = ResidualBlock(filters[2], filters[1])

        self.up2 = nn.ConvTranspose2d(filters[1], filters[0], kernel_size=2, stride=2)
        self.att2 = AttentionBlock(F_g=filters[0], F_l=filters[0], F_int=32)
        self.up_conv2 = ResidualBlock(filters[1], filters[0])

        self.final_conv = nn.Conv2d(filters[0], out_channels, kernel_size=1)

    def forward(self, x):
        # Encoder
        c1 = self.conv1(x)
        p1 = self.maxpool1(c1)

        c2 = self.conv2(p1)
        p2 = self.maxpool2(c2)

        c3 = self.conv3(p2)
        p3 = self.maxpool3(c3)

        c4 = self.conv4(p3)
        p4 = self.maxpool4(c4)

        c5 = self.conv5(p4)

        # Decoder
        u5 = self.up5(c5)
        a5 = self.att5(g=u5, x=c4)
        u5 = torch.cat((a5, u5), dim=1)
        u5 = self.up_conv5(u5)

        u4 = self.up4(u5)
        a4 = self.att4(g=u4, x=c3)
        u4 = torch.cat((a4, u4), dim=1)
        u4 = self.up_conv4(u4)

        u3 = self.up3(u4)
        a3 = self.att3(g=u3, x=c2)
        u3 = torch.cat((a3, u3), dim=1)
        u3 = self.up_conv3(u3)

        u2 = self.up2(u3)
        a2 = self.att2(g=u2, x=c1)
        u2 = torch.cat((a2, u2), dim=1)
        u2 = self.up_conv2(u2)

        outputs = self.final_conv(u2)
        return outputs

# Dataset Preparation
class BrainMRIDataset(Dataset):
    def __init__(self, dataframe, transform=None):
        self.dataframe = dataframe
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        img_path = self.dataframe.iloc[idx, 0]
        mask_path = self.dataframe.iloc[idx, 1]
        image = cv2.imread(img_path)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

        if self.transform:
            augmented = self.transform(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask']

        return image, mask

def get_dataset_dataframe(data_path):
    images = []
    masks = []
    for root, _, files in os.walk(data_path):
        for file in files:
            if file.endswith(".tif") and "mask" not in file:
                images.append(os.path.join(root, file))
                masks.append(os.path.join(root, file.replace(".tif", "_mask.tif")))
    images.sort()
    masks.sort()
    dataframe = pd.DataFrame({"image_path": images, "mask_path": masks})
    return dataframe

# Dice Loss
class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, inputs, targets):
        inputs = torch.sigmoid(inputs)
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        intersection = (inputs * targets).sum()
        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)
        return 1 - dice

# Training Script
def train_model(model_name, model, train_loader, val_loader, train_loss, optimizer, lr_scheduler, num_epochs, device, ckp_path):
    model = model.to(device)
    best_model_wts = copy.deepcopy(model.state_dict())
    best_loss = float('inf')

    loss_history = []
    train_history = []
    val_history = []

    for epoch in range(num_epochs):
        print(f'Epoch {epoch}/{num_epochs - 1}')
        print('-' * 10)

        model.train()
        running_loss = 0.0

        for inputs, targets in tqdm(train_loader):
            inputs = inputs.to(device)
            targets = targets.to(device)
            optimizer.zero_grad()

            outputs = model(inputs)
            loss = train_loss(outputs, targets)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)

        epoch_loss = running_loss / len(train_loader.dataset)
        loss_history.append(epoch_loss)
        print(f'Train Loss: {epoch_loss:.4f}')

        model.eval()
        val_loss = 0.0

        for inputs, targets in val_loader:
            inputs = inputs.to(device)
            targets = targets.to(device)
            with torch.no_grad():
                outputs = model(inputs)
                loss = train_loss(outputs, targets)
                val_loss += loss.item() * inputs.size(0)

        val_epoch_loss = val_loss / len(val_loader.dataset)
        val_history.append(val_epoch_loss)
        print(f'Val Loss: {val_epoch_loss:.4f}')

        if val_epoch_loss < best_loss:
            best_loss = val_epoch_loss
            best_model_wts = copy.deepcopy(model.state_dict())

        if lr_scheduler:
            lr_scheduler.step()

    print(f'Best val Loss: {best_loss:4f}')
    model.load_state_dict(best_model_wts)
    torch.save(model.state_dict(), ckp_path)
    return loss_history, train_history, val_history

# Plotting function
def plot_model_history(model_name, train_history, val_history, num_epochs):
    plt.figure(figsize=(10, 5))
    plt.plot(range(num_epochs), train_history, 'b', label='Training loss')
    plt.plot(range(num_epochs), val_history, 'r', label='Validation loss')
    plt.title(f'Training and validation loss for {model_name}')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Main Execution
data_path = "C:/Users/jervi/Downloads/kaggle_3m"
dataframe = get_dataset_dataframe(data_path)
train_df, val_df = train_test_split(dataframe, test_size=0.2, random_state=42)

# Data augmentation
transform = A.Compose([
    A.RandomCrop(width=256, height=256),
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
    ToTensorV2()
])

train_dataset = BrainMRIDataset(train_df, transform=transform)
val_dataset = BrainMRIDataset(val_df, transform=transform)

train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = UNet3Plus(in_channels=3, out_channels=1)
criterion = DiceLoss()

# Using RMSProp optimizer
optimizer = optim.RMSprop(model.parameters(), lr=1e-4, alpha=0.9)
# Learning rate scheduler
lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)

# Train Model
loss_history, train_history, val_history = train_model(
    model_name="UNet3Plus_Attn_SN",
    model=model,
    train_loader=train_dataloader,
    val_loader=val_dataloader,
    train_loss=criterion,
    optimizer=optimizer,
    lr_scheduler=lr_scheduler,
    num_epochs=3,
    device=device,
    ckp_path="unet3plus_attn_sn.pth"
)

# Plot History
plot_model_history("UNet3Plus_Attn_SN", loss_history, val_history, 3)
