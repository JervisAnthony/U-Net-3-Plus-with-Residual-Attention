{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DO \n",
    "\n",
    "# !pip uninstall Albumentations\n",
    "# !pip install Albumentations==0.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice link: https://towardsdatascience.com/how-accurate-is-image-segmentation-dd448f896388"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping ovencv-python as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless in c:\\users\\jervi\\anaconda3\\envs\\phaseone\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\jervi\\anaconda3\\envs\\phaseone\\lib\\site-packages (from opencv-python-headless) (1.24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall ovencv-python\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.pylab as plt\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from prepareData.prepareData import get_dataset_dataframe\n",
    "# from util.helper import pos_neg_diagnosis, show_aug, train_model, plot_model_history, viz_pred_output\n",
    "# from prepareData import augmentData, customDatasetObject\n",
    "# from model.unet3p_attention import UNet_3Plus_attn\n",
    "# from model.unet3p import UNet3Plus\n",
    "# from model.unet_attention import AttentionUNet\n",
    "# from metrics.diceMetrics import dice_coef_metric, DiceLoss, compute_iou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##prepareData\n",
    "\n",
    "#augmentData\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "PATCH_SIZE = 128\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(width = PATCH_SIZE, height = PATCH_SIZE, p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customDatasetObject\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import cv2 \n",
    "\n",
    "class MRImagingDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.df.iloc[idx, 1])\n",
    "        mask = cv2.imread(self.df.iloc[idx, 2], 0)\n",
    "        \n",
    "        augmented = self.transform(image=image,\n",
    "                                   mask=mask)\n",
    "        \n",
    "        image = augmented[\"image\"]\n",
    "        mask = augmented[\"mask\"]\n",
    "#         mask = np.expand_dims(augmented[\"mask\"], axis=0)# Do not use this\n",
    "        \n",
    "        return image, mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepareData\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_dataset_dataframe(base_path:str):\n",
    "    data = []\n",
    "\n",
    "    for dir_ in os.listdir(base_path):\n",
    "        dir_path = os.path.join(base_path, dir_)\n",
    "        if os.path.isdir(dir_path):\n",
    "            for filename in os.listdir(dir_path):\n",
    "                img_path = os.path.join(dir_path, filename)\n",
    "                data.append([dir_, img_path])\n",
    "        else:\n",
    "            print(f\"[INFO] This is not a dir --> {dir_path}\")\n",
    "            \n",
    "    return pd.DataFrame(data, columns=[\"dir_name\", \"image_path\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Util\n",
    "\n",
    "#helper\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "# from unet3p_attention_capstone.metrics.diceMetrics import dice_coef_metric, compute_iou\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def pos_neg_diagnosis(mask_path):\n",
    "    \"\"\"\n",
    "    To assign 0 or 1 based on the presence of tumor.\n",
    "    \"\"\"\n",
    "    val = np.max(cv2.imread(mask_path))\n",
    "    if val > 0: return 1\n",
    "    else: return 0\n",
    "\n",
    "\n",
    "def show_aug(inputs, nrows=5, ncols=5, norm=False):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplots_adjust(wspace=0., hspace=0.)\n",
    "    i_ = 0\n",
    "    \n",
    "    if len(inputs) > 25:\n",
    "        inputs = inputs[:25]\n",
    "        \n",
    "    for idx in range(len(inputs)):\n",
    "    \n",
    "        # normalization\n",
    "        if norm:           \n",
    "            img = inputs[idx].numpy().transpose(1,2,0)\n",
    "            mean = [0.485, 0.456, 0.406]\n",
    "            std = [0.229, 0.224, 0.225] \n",
    "            img = (img*std+mean).astype(np.float32)\n",
    "            \n",
    "        else:\n",
    "            img = inputs[idx].numpy().astype(np.float32)\n",
    "            img = img[0,:,:]\n",
    "        \n",
    "        plt.subplot(nrows, ncols, i_+1)\n",
    "        plt.imshow(img); \n",
    "        plt.axis('off')\n",
    " \n",
    "        i_ += 1\n",
    "        \n",
    "    return plt.show()\n",
    "\n",
    "def save_ckp(state, is_best, checkpoint_dir, best_model_dir):\n",
    "    f_path = checkpoint_dir + '\\\\checkpoint.pt'\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_dir + '\\\\best_model.pt'\n",
    "        shutil.copyfile(f_path, best_fpath)\n",
    "\n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print('Previously trained model weights state_dict loaded...')\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print('Previously trained optimizer state_dict loaded...')\n",
    "    last_epoch = checkpoint['epoch']\n",
    "    print(f\"Previously trained for {last_epoch} number of epochs...\")\n",
    "    return model, optimizer, last_epoch\n",
    "\n",
    "def train_model(model_name, model, train_loader, val_loader, train_loss, optimizer, lr_scheduler, num_epochs, device, ckp_path:str=None):\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"total params of {model_name} model: {pytorch_total_params}\")\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"trainable params of {model_name} model: {pytorch_total_params}\")\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    start_epoch=0\n",
    "\n",
    "    if ckp_path is not None:\n",
    "        model, optimizer, last_epoch = load_ckp(ckp_path, model, optimizer)\n",
    "        start_epoch = last_epoch + 1\n",
    "        print(f\"Train for {num_epochs} more epochs...\")\n",
    "\n",
    "    print(f\"[INFO] Model is initializing... {model_name}\")\n",
    "\n",
    "    checkpoint_dir = f\"C:\\\\Users\\\\Ryan\\\\Documents\\\\checkpoints\\\\{model_name}\"\n",
    "    best_model_dir = f\"{checkpoint_dir}\\\\{model_name}_best\"\n",
    "    #best_model_dir = f\"{checkpoint_dir}\\\\best\"\n",
    "\n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    \n",
    "    mean_loss_ = 999\n",
    "    \n",
    "    for epoch in range(start_epoch, start_epoch+num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        losses = []\n",
    "        train_iou = []\n",
    "        \n",
    "        for i_step, (data, target) in enumerate(tqdm(train_loader)):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            \n",
    "            out_cut = np.copy(outputs.data.cpu().numpy())\n",
    "            out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n",
    "            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n",
    "            \n",
    "            train_dice = dice_coef_metric(out_cut, target.data.cpu().numpy())\n",
    "            \n",
    "            loss = train_loss(outputs, target)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            train_iou.append(train_dice)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "        val_mean_iou = compute_iou(model, val_loader, device=device)\n",
    "        \n",
    "        mean_loss = np.array(losses).mean()\n",
    "        scheduler.step(mean_loss)\n",
    "\n",
    "        loss_history.append(mean_loss)\n",
    "        train_history.append(np.array(train_iou).mean())\n",
    "        val_history.append(val_mean_iou)\n",
    "\n",
    "        checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': np.array(losses).mean(),\n",
    "                    }\n",
    "        save_ckp(checkpoint, False, checkpoint_dir, best_model_dir)\n",
    "\n",
    "        if loss<mean_loss_:\n",
    "            save_ckp(checkpoint, True, checkpoint_dir, best_model_dir)\n",
    "            mean_loss_ = loss\n",
    "                # torch.save({\n",
    "                #     'epoch': epoch,\n",
    "                #     'model_state_dict': model.state_dict(),\n",
    "                #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "                #     'loss': np.array(losses).mean(),\n",
    "                #     }, f\"{path[:-3]}_best.pt\")\n",
    "        \n",
    "        \n",
    "\n",
    "#         print(\"losses:\", np.array(losses))\n",
    "#         print(\"iou:\", np.array(train_iou))\n",
    "        print(\"Epoch [%d]\" % (epoch))\n",
    "        print(\"Mean loss on train:\", np.array(losses).mean(), \n",
    "              \"\\nMean DICE on train:\", np.array(train_iou).mean(), \n",
    "              \"\\nMean DICE on validation:\", val_mean_iou)\n",
    "        \n",
    "    return loss_history, train_history, val_history\n",
    "\n",
    "def plot_model_history(model_name,\n",
    "                    train_history, val_history, \n",
    "                    num_epochs):\n",
    "\n",
    "    x = np.arange(num_epochs)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, train_history, label='train dice', lw=3, c=\"springgreen\")\n",
    "    plt.plot(x, val_history, label='validation dice', lw=3, c=\"deeppink\")\n",
    "\n",
    "    plt.title(f\"{model_name}\", fontsize=15)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel(\"Epoch\", fontsize=15)\n",
    "    plt.ylabel(\"DICE\", fontsize=15)\n",
    "\n",
    "    fn = str(int(time.time())) + \".png\"\n",
    "    plt.savefig(f'{model_name}/{model_name}_dice.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def viz_pred_output(model, loader, idx, test_dataset, device=\"mps\", threshold=0.3):\n",
    "    valloss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "#         for i_step, (data, target) in enumerate(loader):\n",
    "        target = torch.tensor(test_dataset[idx][1])\n",
    "        data = torch.tensor(test_dataset[idx][0])\n",
    "\n",
    "        data = data.to(device).unsqueeze(0)\n",
    "        target = target.to(device).unsqueeze(0)\n",
    "\n",
    "        outputs = model(data)\n",
    "\n",
    "        out_cut = np.copy(outputs.data.cpu().numpy())\n",
    "        out_cut[np.nonzero(out_cut < threshold)] = 0.0\n",
    "        out_cut[np.nonzero(out_cut >= threshold)] = 1.0\n",
    "\n",
    "        f, axarr = plt.subplots(1,2)\n",
    "#             axarr[0,0].imshow(image_datas[0])\n",
    "#             axarr[0,1].imshow(image_datas[1])\n",
    "\n",
    "        targ = target.data.cpu().numpy()[0][0]\n",
    "        target_img = cv2.merge((targ,targ,targ))\n",
    "        axarr[0].imshow(target_img)\n",
    "\n",
    "        op = out_cut[0][0]\n",
    "        axarr[1].imshow(op)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Models</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlockWithAttention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlockWithAttention, self).__init__()\n",
    "\n",
    "        # Convolutional layers for the main path\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Batch normalization for the main path\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),  # Adjust channels for attention\n",
    "            nn.Sigmoid()  # Sigmoid activation for attention map\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main path\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_map = self.attention(x)\n",
    "        out = out * attention_map\n",
    "\n",
    "        # Skip connection\n",
    "        out += residual\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #attention_mechanism\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class AttentionBlock(nn.Module):\n",
    "#     def __init__(self, f_g, f_l, f_int):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.w_g = nn.Sequential(\n",
    "#                                 nn.Conv2d(f_g, f_int,\n",
    "#                                          kernel_size=1, stride=1,\n",
    "#                                          padding=0, bias=True),\n",
    "#                                 nn.BatchNorm2d(f_int)\n",
    "#         )\n",
    "        \n",
    "#         self.w_x = nn.Sequential(\n",
    "#                                 nn.Conv2d(f_l, f_int,\n",
    "#                                          kernel_size=1, stride=1,\n",
    "#                                          padding=0, bias=True),\n",
    "#                                 nn.BatchNorm2d(f_int)\n",
    "#         )\n",
    "        \n",
    "#         self.psi = nn.Sequential(\n",
    "#                                 nn.Conv2d(f_int, 1,\n",
    "#                                          kernel_size=1, stride=1,\n",
    "#                                          padding=0,  bias=True),\n",
    "#                                 nn.BatchNorm2d(1),\n",
    "#                                 nn.Sigmoid(),\n",
    "#         )\n",
    "        \n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "#     def forward(self, g, x):\n",
    "#         g1 = self.w_g(g)\n",
    "#         x1 = self.w_x(x)\n",
    "#         psi = self.relu(g1+x1)\n",
    "#         psi = self.psi(psi)\n",
    "        \n",
    "#         return psi*x\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. UNet with Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #UNet_with_Attention_Mechanism\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import init\n",
    "# import torch.nn.functional as F\n",
    "# from . import attention\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import init\n",
    "\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, ch_in, ch_out):\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#                                   nn.Conv2d(ch_in, ch_out,\n",
    "#                                             kernel_size=3, stride=1,\n",
    "#                                             padding=1, bias=True),\n",
    "#                                   nn.BatchNorm2d(ch_out),\n",
    "#                                   nn.ReLU(inplace=True),\n",
    "#                                   nn.Conv2d(ch_out, ch_out,\n",
    "#                                             kernel_size=3, stride=1,\n",
    "#                                             padding=1, bias=True),\n",
    "#                                   nn.BatchNorm2d(ch_out),\n",
    "#                                   nn.ReLU(inplace=True),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         return x\n",
    "\n",
    "# class UpConvBlock(nn.Module):\n",
    "#     def __init__(self, ch_in, ch_out):\n",
    "#         super().__init__()\n",
    "#         self.up = nn.Sequential(\n",
    "#                                 nn.Upsample(scale_factor=2),\n",
    "#                                 nn.Conv2d(ch_in, ch_out,\n",
    "#                                          kernel_size=3,stride=1,\n",
    "#                                          padding=1, bias=True),\n",
    "#                                 nn.BatchNorm2d(ch_out),\n",
    "#                                 nn.ReLU(inplace=True),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = x = self.up(x)\n",
    "#         return x\n",
    "\n",
    "# class AttentionUNet(nn.Module):\n",
    "#     def __init__(self, n_classes=1, in_channel=3, out_channel=1):\n",
    "#         super().__init__() \n",
    "        \n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.conv1 = ConvBlock(ch_in=in_channel, ch_out=64)\n",
    "#         self.conv2 = ConvBlock(ch_in=64, ch_out=128)\n",
    "#         self.conv3 = ConvBlock(ch_in=128, ch_out=256)\n",
    "#         self.conv4 = ConvBlock(ch_in=256, ch_out=512)\n",
    "#         self.conv5 = ConvBlock(ch_in=512, ch_out=1024)\n",
    "        \n",
    "#         self.up5 = UpConvBlock(ch_in=1024, ch_out=512)\n",
    "#         self.att5 = attention.AttentionBlock(f_g=512, f_l=512, f_int=256)\n",
    "#         self.upconv5 = ConvBlock(ch_in=1024, ch_out=512)\n",
    "        \n",
    "#         self.up4 = UpConvBlock(ch_in=512, ch_out=256)\n",
    "#         self.att4 = attention.AttentionBlock(f_g=256, f_l=256, f_int=128)\n",
    "#         self.upconv4 = ConvBlock(ch_in=512, ch_out=256)\n",
    "        \n",
    "#         self.up3 = UpConvBlock(ch_in=256, ch_out=128)\n",
    "#         self.att3 = attention.AttentionBlock(f_g=128, f_l=128, f_int=64)\n",
    "#         self.upconv3 = ConvBlock(ch_in=256, ch_out=128)\n",
    "        \n",
    "#         self.up2 = UpConvBlock(ch_in=128, ch_out=64)\n",
    "#         self.att2 = attention.AttentionBlock(f_g=64, f_l=64, f_int=32)\n",
    "#         self.upconv2 = ConvBlock(ch_in=128, ch_out=64)\n",
    "        \n",
    "#         self.conv_1x1 = nn.Conv2d(64, out_channel,\n",
    "#                                   kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # encoder\n",
    "#         x1 = self.conv1(x)\n",
    "        \n",
    "#         x2 = self.maxpool(x1)\n",
    "#         x2 = self.conv2(x2)\n",
    "        \n",
    "#         x3 = self.maxpool(x2)\n",
    "#         x3 = self.conv3(x3)\n",
    "        \n",
    "#         x4 = self.maxpool(x3)\n",
    "#         x4 = self.conv4(x4)\n",
    "        \n",
    "#         x5 = self.maxpool(x4)\n",
    "#         x5 = self.conv5(x5)\n",
    "        \n",
    "#         # decoder + concat\n",
    "#         d5 = self.up5(x5)\n",
    "#         x4 = self.att5(g=d5, x=x4)\n",
    "#         d5 = torch.concat((x4, d5), dim=1)\n",
    "#         d5 = self.upconv5(d5)\n",
    "        \n",
    "#         d4 = self.up4(d5)\n",
    "#         x3 = self.att4(g=d4, x=x3)\n",
    "#         d4 = torch.concat((x3, d4), dim=1)\n",
    "#         d4 = self.upconv4(d4)\n",
    "        \n",
    "#         d3 = self.up3(d4)\n",
    "#         x2 = self.att3(g=d3, x=x2)\n",
    "#         d3 = torch.concat((x2, d3), dim=1)\n",
    "#         d3 = self.upconv3(d3)\n",
    "        \n",
    "#         d2 = self.up2(d3)\n",
    "#         x1 = self.att2(g=d2, x=x1)\n",
    "#         d2 = torch.concat((x1, d2), dim=1)\n",
    "#         d2 = self.upconv2(d2)\n",
    "        \n",
    "#         d1 = self.conv_1x1(d2)\n",
    "        \n",
    "#         return d1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. UNet3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #UNet3+\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import init\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import init\n",
    "\n",
    "# def weights_init_normal(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     #print(classname)\n",
    "#     if classname.find('Conv') != -1:\n",
    "#         init.normal_(m.weight.data, 0.0, 0.02)\n",
    "#     elif classname.find('Linear') != -1:\n",
    "#         init.normal_(m.weight.data, 0.0, 0.02)\n",
    "#     elif classname.find('BatchNorm') != -1:\n",
    "#         init.normal_(m.weight.data, 1.0, 0.02)\n",
    "#         init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "# def weights_init_xavier(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     #print(classname)\n",
    "#     if classname.find('Conv') != -1:\n",
    "#         init.xavier_normal_(m.weight.data, gain=1)\n",
    "#     elif classname.find('Linear') != -1:\n",
    "#         init.xavier_normal_(m.weight.data, gain=1)\n",
    "#     elif classname.find('BatchNorm') != -1:\n",
    "#         init.normal_(m.weight.data, 1.0, 0.02)\n",
    "#         init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "# def weights_init_kaiming(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     #print(classname)\n",
    "#     if classname.find('Conv') != -1:\n",
    "#         init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "#     elif classname.find('Linear') != -1:\n",
    "#         init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "#     elif classname.find('BatchNorm') != -1:\n",
    "#         init.normal_(m.weight.data, 1.0, 0.02)\n",
    "#         init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "# def weights_init_orthogonal(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     #print(classname)\n",
    "#     if classname.find('Conv') != -1:\n",
    "#         init.orthogonal_(m.weight.data, gain=1)\n",
    "#     elif classname.find('Linear') != -1:\n",
    "#         init.orthogonal_(m.weight.data, gain=1)\n",
    "#     elif classname.find('BatchNorm') != -1:\n",
    "#         init.normal_(m.weight.data, 1.0, 0.02)\n",
    "#         init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "# def init_weights(net, init_type='normal'):\n",
    "#     #print('initialization method [%s]' % init_type)\n",
    "#     if init_type == 'normal':\n",
    "#         net.apply(weights_init_normal)\n",
    "#     elif init_type == 'xavier':\n",
    "#         net.apply(weights_init_xavier)\n",
    "#     elif init_type == 'kaiming':\n",
    "#         net.apply(weights_init_kaiming)\n",
    "#     elif init_type == 'orthogonal':\n",
    "#         net.apply(weights_init_orthogonal)\n",
    "#     else:\n",
    "#         raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "\n",
    "# class unetConv2(nn.Module):\n",
    "#     def __init__(self, in_size, out_size, is_batchnorm, n=2, ks=3, stride=1, padding=1):\n",
    "#         super(unetConv2, self).__init__()\n",
    "#         self.n = n\n",
    "#         self.ks = ks\n",
    "#         self.stride = stride\n",
    "#         self.padding = padding\n",
    "#         s = stride\n",
    "#         p = padding\n",
    "#         if is_batchnorm:\n",
    "#             for i in range(1, n + 1):\n",
    "#                 conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
    "#                                      nn.BatchNorm2d(out_size),\n",
    "#                                      nn.ReLU(inplace=True), )\n",
    "#                 setattr(self, 'conv%d' % i, conv)\n",
    "#                 in_size = out_size\n",
    "\n",
    "#         else:\n",
    "#             for i in range(1, n + 1):\n",
    "#                 conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
    "#                                      nn.ReLU(inplace=True), )\n",
    "#                 setattr(self, 'conv%d' % i, conv)\n",
    "#                 in_size = out_size\n",
    "\n",
    "#         # initialise the blocks\n",
    "#         for m in self.children():\n",
    "#             init_weights(m, init_type='kaiming')\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         x = inputs\n",
    "#         for i in range(1, self.n + 1):\n",
    "#             conv = getattr(self, 'conv%d' % i)\n",
    "#             x = conv(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "# class UNet3Plus(nn.Module):\n",
    "#     def __init__(self, n_channels=3, n_classes=1, bilinear=True, feature_scale=4,\n",
    "#                  is_deconv=True, is_batchnorm=True):\n",
    "#         super(UNet3Plus, self).__init__()\n",
    "#         self.n_channels = n_channels\n",
    "#         self.n_classes = n_classes\n",
    "#         self.bilinear = bilinear\n",
    "#         self.feature_scale = feature_scale\n",
    "#         self.is_deconv = is_deconv\n",
    "#         self.is_batchnorm = is_batchnorm\n",
    "#         filters = [64, 128, 256, 512, 1024]\n",
    "\n",
    "#         ## -------------Encoder--------------\n",
    "#         self.conv1 = unetConv2(self.n_channels, filters[0], self.is_batchnorm)\n",
    "#         self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "#         self.conv2 = unetConv2(filters[0], filters[1], self.is_batchnorm)\n",
    "#         self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "#         self.conv3 = unetConv2(filters[1], filters[2], self.is_batchnorm)\n",
    "#         self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "#         self.conv4 = unetConv2(filters[2], filters[3], self.is_batchnorm)\n",
    "#         self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "#         self.conv5 = unetConv2(filters[3], filters[4], self.is_batchnorm)\n",
    "\n",
    "#         ## -------------Decoder--------------\n",
    "#         self.CatChannels = filters[0]\n",
    "#         self.CatBlocks = 5\n",
    "#         self.UpChannels = self.CatChannels * self.CatBlocks\n",
    "\n",
    "#         '''stage 4d'''\n",
    "#         # h1->320*320, hd4->40*40, Pooling 8 times\n",
    "#         self.h1_PT_hd4 = nn.MaxPool2d(8, 8, ceil_mode=True)\n",
    "#         self.h1_PT_hd4_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "#         self.h1_PT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.h1_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # h2->160*160, hd4->40*40, Pooling 4 times\n",
    "#         self.h2_PT_hd4 = nn.MaxPool2d(4, 4, ceil_mode=True)\n",
    "#         self.h2_PT_hd4_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "#         self.h2_PT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.h2_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # h3->80*80, hd4->40*40, Pooling 2 times\n",
    "#         self.h3_PT_hd4 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "#         self.h3_PT_hd4_conv = nn.Conv2d(filters[2], self.CatChannels, 3, padding=1)\n",
    "#         self.h3_PT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.h3_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # h4->40*40, hd4->40*40, Concatenation\n",
    "#         self.h4_Cat_hd4_conv = nn.Conv2d(filters[3], self.CatChannels, 3, padding=1)\n",
    "#         self.h4_Cat_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.h4_Cat_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # hd5->20*20, hd4->40*40, Upsample 2 times\n",
    "#         self.hd5_UT_hd4 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "#         self.hd5_UT_hd4_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "#         self.hd5_UT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.hd5_UT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # fusion(h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4, hd5_UT_hd4)\n",
    "#         self.conv4d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "#         self.bn4d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "#         self.relu4d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "#         '''stage 3d'''\n",
    "#         # h1->320*320, hd3->80*80, Pooling 4 times\n",
    "#         self.h1_PT_hd3 = nn.MaxPool2d(4, 4, ceil_mode=True)\n",
    "#         self.h1_PT_hd3_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "#         self.h1_PT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.h1_PT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # h2->160*160, hd3->80*80, Pooling 2 times\n",
    "#         self.h2_PT_hd3 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "#         self.h2_PT_hd3_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "#         self.h2_PT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.h2_PT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # h3->80*80, hd3->80*80, Concatenation\n",
    "#         self.h3_Cat_hd3_conv = nn.Conv2d(filters[2], self.CatChannels, 3, padding=1)\n",
    "#         self.h3_Cat_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.h3_Cat_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # hd4->40*40, hd4->80*80, Upsample 2 times\n",
    "#         self.hd4_UT_hd3 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "#         self.hd4_UT_hd3_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "#         self.hd4_UT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.hd4_UT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # hd5->20*20, hd4->80*80, Upsample 4 times\n",
    "#         self.hd5_UT_hd3 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "#         self.hd5_UT_hd3_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "#         self.hd5_UT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.hd5_UT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # fusion(h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, hd5_UT_hd3)\n",
    "#         self.conv3d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "#         self.bn3d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "#         self.relu3d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "#         '''stage 2d '''\n",
    "#         # h1->320*320, hd2->160*160, Pooling 2 times\n",
    "#         self.h1_PT_hd2 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "#         self.h1_PT_hd2_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "#         self.h1_PT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.h1_PT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # h2->160*160, hd2->160*160, Concatenation\n",
    "#         self.h2_Cat_hd2_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "#         self.h2_Cat_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.h2_Cat_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # hd3->80*80, hd2->160*160, Upsample 2 times\n",
    "#         self.hd3_UT_hd2 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "#         self.hd3_UT_hd2_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "#         self.hd3_UT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.hd3_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # hd4->40*40, hd2->160*160, Upsample 4 times\n",
    "#         self.hd4_UT_hd2 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "#         self.hd4_UT_hd2_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "#         self.hd4_UT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.hd4_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # hd5->20*20, hd2->160*160, Upsample 8 times\n",
    "#         self.hd5_UT_hd2 = nn.Upsample(scale_factor=8, mode='bilinear')  # 14*14\n",
    "#         self.hd5_UT_hd2_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "#         self.hd5_UT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.hd5_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # fusion(h1_PT_hd2, h2_Cat_hd2, hd3_UT_hd2, hd4_UT_hd2, hd5_UT_hd2)\n",
    "#         self.conv2d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "#         self.bn2d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "#         self.relu2d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "#         '''stage 1d'''\n",
    "#         # h1->320*320, hd1->320*320, Concatenation\n",
    "#         self.h1_Cat_hd1_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "#         self.h1_Cat_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.h1_Cat_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # hd2->160*160, hd1->320*320, Upsample 2 times\n",
    "#         self.hd2_UT_hd1 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "#         self.hd2_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "#         self.hd2_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.hd2_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # hd3->80*80, hd1->320*320, Upsample 4 times\n",
    "#         self.hd3_UT_hd1 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "#         self.hd3_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "#         self.hd3_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.hd3_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # hd4->40*40, hd1->320*320, Upsample 8 times\n",
    "#         self.hd4_UT_hd1 = nn.Upsample(scale_factor=8, mode='bilinear')  # 14*14\n",
    "#         self.hd4_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "#         self.hd4_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.hd4_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # hd5->20*20, hd1->320*320, Upsample 16 times\n",
    "#         self.hd5_UT_hd1 = nn.Upsample(scale_factor=16, mode='bilinear')  # 14*14\n",
    "#         self.hd5_UT_hd1_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "#         self.hd5_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "#         self.hd5_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # fusion(h1_Cat_hd1, hd2_UT_hd1, hd3_UT_hd1, hd4_UT_hd1, hd5_UT_hd1)\n",
    "#         self.conv1d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "#         self.bn1d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "#         self.relu1d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # output\n",
    "#         self.outconv1 = nn.Conv2d(self.UpChannels, n_classes, 3, padding=1)\n",
    "\n",
    "#         # initialise weights\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 init_weights(m, init_type='kaiming')\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 init_weights(m, init_type='kaiming')\n",
    "\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         ## -------------Encoder-------------\n",
    "#         h1 = self.conv1(inputs)  # h1->320*320*64\n",
    "\n",
    "#         h2 = self.maxpool1(h1)\n",
    "#         h2 = self.conv2(h2)  # h2->160*160*128\n",
    "\n",
    "#         h3 = self.maxpool2(h2)\n",
    "#         h3 = self.conv3(h3)  # h3->80*80*256\n",
    "\n",
    "#         h4 = self.maxpool3(h3)\n",
    "#         h4 = self.conv4(h4)  # h4->40*40*512\n",
    "\n",
    "#         h5 = self.maxpool4(h4)\n",
    "#         hd5 = self.conv5(h5)  # h5->20*20*1024\n",
    "\n",
    "#         ## -------------Decoder-------------\n",
    "#         h1_PT_hd4 = self.h1_PT_hd4_relu(self.h1_PT_hd4_bn(self.h1_PT_hd4_conv(self.h1_PT_hd4(h1))))\n",
    "#         h2_PT_hd4 = self.h2_PT_hd4_relu(self.h2_PT_hd4_bn(self.h2_PT_hd4_conv(self.h2_PT_hd4(h2))))\n",
    "#         h3_PT_hd4 = self.h3_PT_hd4_relu(self.h3_PT_hd4_bn(self.h3_PT_hd4_conv(self.h3_PT_hd4(h3))))\n",
    "#         h4_Cat_hd4 = self.h4_Cat_hd4_relu(self.h4_Cat_hd4_bn(self.h4_Cat_hd4_conv(h4)))\n",
    "#         hd5_UT_hd4 = self.hd5_UT_hd4_relu(self.hd5_UT_hd4_bn(self.hd5_UT_hd4_conv(self.hd5_UT_hd4(hd5))))\n",
    "#         hd4 = self.relu4d_1(self.bn4d_1(self.conv4d_1(torch.cat((h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4, hd5_UT_hd4), 1)))) # hd4->40*40*UpChannels\n",
    "\n",
    "#         h1_PT_hd3 = self.h1_PT_hd3_relu(self.h1_PT_hd3_bn(self.h1_PT_hd3_conv(self.h1_PT_hd3(h1))))\n",
    "#         h2_PT_hd3 = self.h2_PT_hd3_relu(self.h2_PT_hd3_bn(self.h2_PT_hd3_conv(self.h2_PT_hd3(h2))))\n",
    "#         h3_Cat_hd3 = self.h3_Cat_hd3_relu(self.h3_Cat_hd3_bn(self.h3_Cat_hd3_conv(h3)))\n",
    "#         hd4_UT_hd3 = self.hd4_UT_hd3_relu(self.hd4_UT_hd3_bn(self.hd4_UT_hd3_conv(self.hd4_UT_hd3(hd4))))\n",
    "#         hd5_UT_hd3 = self.hd5_UT_hd3_relu(self.hd5_UT_hd3_bn(self.hd5_UT_hd3_conv(self.hd5_UT_hd3(hd5))))\n",
    "#         hd3 = self.relu3d_1(self.bn3d_1(self.conv3d_1(torch.cat((h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, hd5_UT_hd3), 1)))) # hd3->80*80*UpChannels\n",
    "\n",
    "#         h1_PT_hd2 = self.h1_PT_hd2_relu(self.h1_PT_hd2_bn(self.h1_PT_hd2_conv(self.h1_PT_hd2(h1))))\n",
    "#         h2_Cat_hd2 = self.h2_Cat_hd2_relu(self.h2_Cat_hd2_bn(self.h2_Cat_hd2_conv(h2)))\n",
    "#         hd3_UT_hd2 = self.hd3_UT_hd2_relu(self.hd3_UT_hd2_bn(self.hd3_UT_hd2_conv(self.hd3_UT_hd2(hd3))))\n",
    "#         hd4_UT_hd2 = self.hd4_UT_hd2_relu(self.hd4_UT_hd2_bn(self.hd4_UT_hd2_conv(self.hd4_UT_hd2(hd4))))\n",
    "#         hd5_UT_hd2 = self.hd5_UT_hd2_relu(self.hd5_UT_hd2_bn(self.hd5_UT_hd2_conv(self.hd5_UT_hd2(hd5))))\n",
    "#         hd2 = self.relu2d_1(self.bn2d_1(self.conv2d_1(torch.cat((h1_PT_hd2, h2_Cat_hd2, hd3_UT_hd2, hd4_UT_hd2, hd5_UT_hd2), 1)))) # hd2->160*160*UpChannels\n",
    "\n",
    "#         h1_Cat_hd1 = self.h1_Cat_hd1_relu(self.h1_Cat_hd1_bn(self.h1_Cat_hd1_conv(h1)))\n",
    "#         hd2_UT_hd1 = self.hd2_UT_hd1_relu(self.hd2_UT_hd1_bn(self.hd2_UT_hd1_conv(self.hd2_UT_hd1(hd2))))\n",
    "#         hd3_UT_hd1 = self.hd3_UT_hd1_relu(self.hd3_UT_hd1_bn(self.hd3_UT_hd1_conv(self.hd3_UT_hd1(hd3))))\n",
    "#         hd4_UT_hd1 = self.hd4_UT_hd1_relu(self.hd4_UT_hd1_bn(self.hd4_UT_hd1_conv(self.hd4_UT_hd1(hd4))))\n",
    "#         hd5_UT_hd1 = self.hd5_UT_hd1_relu(self.hd5_UT_hd1_bn(self.hd5_UT_hd1_conv(self.hd5_UT_hd1(hd5))))\n",
    "#         hd1 = self.relu1d_1(self.bn1d_1(self.conv1d_1(torch.cat((h1_Cat_hd1, hd2_UT_hd1, hd3_UT_hd1, hd4_UT_hd1, hd5_UT_hd1), 1)))) # hd1->320*320*UpChannels\n",
    "\n",
    "#         d1 = self.outconv1(hd1)  # d1->320*320*n_classes\n",
    "#         return F.sigmoid(d1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. UNet3+ with Residual Attention Mechanism (UNet_3Plus_ResAttn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNet3+_with_Attention_Mechanism\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "# from . import attention\n",
    "\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, ch_in, ch_out):\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#                                   nn.Conv2d(ch_in, ch_out,\n",
    "#                                             kernel_size=3, stride=1,\n",
    "#                                             padding=1, bias=True),\n",
    "#                                   nn.BatchNorm2d(ch_out),\n",
    "#                                   nn.ReLU(inplace=True),\n",
    "#                                   nn.Conv2d(ch_out, ch_out,\n",
    "#                                             kernel_size=3, stride=1,\n",
    "#                                             padding=1, bias=True),\n",
    "#                                   nn.BatchNorm2d(ch_out),\n",
    "#                                   nn.ReLU(inplace=True),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         return x\n",
    "\n",
    "# class UpConvBlock(nn.Module):\n",
    "#     def __init__(self, ch_in, ch_out):\n",
    "#         super().__init__()\n",
    "#         self.up = nn.Sequential(\n",
    "#                                 nn.Upsample(scale_factor=2),\n",
    "#                                 nn.Conv2d(ch_in, ch_out,\n",
    "#                                          kernel_size=3,stride=1,\n",
    "#                                          padding=1, bias=True),\n",
    "#                                 nn.BatchNorm2d(ch_out),\n",
    "#                                 nn.ReLU(inplace=True),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = x = self.up(x)\n",
    "#         return x\n",
    "\n",
    "class unetConv2(nn.Module):\n",
    "    def __init__(self, in_size, out_size, is_batchnorm, n=2, ks=3, stride=1, padding=1):\n",
    "        super(unetConv2, self).__init__()\n",
    "        self.n = n\n",
    "        self.ks = ks\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        s = stride\n",
    "        p = padding\n",
    "        if is_batchnorm:\n",
    "            for i in range(1, n + 1):\n",
    "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
    "                                     SwitchNorm2d(out_size),\n",
    "                                     nn.ReLU(inplace=True), )\n",
    "                setattr(self, 'conv%d' % i, conv)\n",
    "                in_size = out_size\n",
    "\n",
    "        else:\n",
    "            for i in range(1, n + 1):\n",
    "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
    "                                     nn.ReLU(inplace=True), )\n",
    "                setattr(self, 'conv%d' % i, conv)\n",
    "                in_size = out_size\n",
    "\n",
    "        # initialise the blocks\n",
    "        for m in self.children():\n",
    "            init_weights(m, init_type='kaiming')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        for i in range(1, self.n + 1):\n",
    "            conv = getattr(self, 'conv%d' % i)\n",
    "            x = conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('SwitchNorm2d') != -1:\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def weights_init_xavier(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.xavier_normal_(m.weight.data, gain=1)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.xavier_normal_(m.weight.data, gain=1)\n",
    "    elif classname.find('SwitchNorm2d') != -1:\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def weights_init_kaiming(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "    elif classname.find('SwitchNorm2d') != -1:\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def weights_init_orthogonal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.orthogonal_(m.weight.data, gain=1)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.orthogonal_(m.weight.data, gain=1)\n",
    "    elif classname.find('SwitchNorm2d') != -1:\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def init_weights(net, init_type='normal'):\n",
    "    #print('initialization method [%s]' % init_type)\n",
    "    if init_type == 'normal':\n",
    "        net.apply(weights_init_normal)\n",
    "    elif init_type == 'xavier':\n",
    "        net.apply(weights_init_xavier)\n",
    "    elif init_type == 'kaiming':\n",
    "        net.apply(weights_init_kaiming)\n",
    "    elif init_type == 'orthogonal':\n",
    "        net.apply(weights_init_orthogonal)\n",
    "    else:\n",
    "        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "\n",
    "\n",
    "\n",
    "class UNet_3Plus_ResAttn(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, n_classes=1, feature_scale=4, is_deconv=True, is_batchnorm=True):\n",
    "        super(UNet_3Plus_ResAttn, self).__init__()\n",
    "        self.is_deconv = is_deconv\n",
    "        self.in_channels = in_channels\n",
    "        self.is_batchnorm = is_batchnorm\n",
    "        self.feature_scale = feature_scale\n",
    "\n",
    "        filters = [64, 128, 256, 512, 1024]\n",
    "\n",
    "        ## -------------Encoder--------------\n",
    "        self.conv1 = unetConv2(self.in_channels, filters[0], self.is_batchnorm)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = unetConv2(filters[0], filters[1], self.is_batchnorm)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv3 = unetConv2(filters[1], filters[2], self.is_batchnorm)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv4 = unetConv2(filters[2], filters[3], self.is_batchnorm)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv5 = unetConv2(filters[3], filters[4], self.is_batchnorm)\n",
    "\n",
    "        ## -------------Decoder--------------\n",
    "        self.CatChannels = filters[0]\n",
    "        self.CatBlocks = 5\n",
    "        self.UpChannels = self.CatChannels * self.CatBlocks\n",
    "\n",
    "        '''stage 4d'''\n",
    "        # h1->320*320, hd4->40*40, Pooling 8 times\n",
    "        self.h1_PT_hd4 = nn.MaxPool2d(8, 8, ceil_mode=True)\n",
    "        self.h1_PT_hd4_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_PT_hd4_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.h1_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "        # self.att4 = attention.AttentionBlock(f_g=256, f_l=64, f_int=256)\n",
    "        self.att4 = ResidualBlockWithAttention(256,256)\n",
    "\n",
    "        # h2->160*160, hd4->40*40, Pooling 4 times\n",
    "        self.h2_PT_hd4 = nn.MaxPool2d(4, 4, ceil_mode=True)\n",
    "        self.h2_PT_hd4_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "        self.h2_PT_hd4_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.h2_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "        # h3->80*80, hd4->40*40, Pooling 2 times\n",
    "        self.h3_PT_hd4 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.h3_PT_hd4_conv = nn.Conv2d(filters[2], self.CatChannels, 3, padding=1)\n",
    "        self.h3_PT_hd4_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.h3_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h4->40*40, hd4->40*40, Concatenation\n",
    "        self.h4_Cat_hd4_conv = nn.Conv2d(filters[3], self.CatChannels, 3, padding=1)\n",
    "        self.h4_Cat_hd4_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.h4_Cat_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd4->40*40, Upsample 2 times\n",
    "        self.hd5_UT_hd4 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd4_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd4_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4, hd5_UT_hd4)\n",
    "#         self.conv4d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "# modified\n",
    "#         self.conv4d_1 = nn.Conv2d(128, self.UpChannels, 3, padding=1)  # 16 # if only 2 inputs concatenated(each input 64 size) \n",
    "        self.conv4d_1 = nn.Conv2d(384, self.UpChannels, 3, padding=1)  # 16 (6*64 = 384)\n",
    "    \n",
    "        self.bn4d_1 = SwitchNorm2d(self.UpChannels)\n",
    "        self.relu4d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        '''stage 3d'''\n",
    "        # self.att3 = attention.AttentionBlock(f_g=256, f_l=64, f_int=256)\n",
    "        self.att3 = ResidualBlockWithAttention(256,256)\n",
    "\n",
    "        # h1->320*320, hd3->80*80, Pooling 4 times\n",
    "        self.h1_PT_hd3 = nn.MaxPool2d(4, 4, ceil_mode=True)\n",
    "        self.h1_PT_hd3_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_PT_hd3_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.h1_PT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h2->160*160, hd3->80*80, Pooling 2 times\n",
    "        self.h2_PT_hd3 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.h2_PT_hd3_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "        self.h2_PT_hd3_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.h2_PT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h3->80*80, hd3->80*80, Concatenation\n",
    "        self.h3_Cat_hd3_conv = nn.Conv2d(filters[2], self.CatChannels, 3, padding=1)\n",
    "        self.h3_Cat_hd3_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.h3_Cat_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd4->40*40, hd4->80*80, Upsample 2 times\n",
    "        self.hd4_UT_hd3 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd4_UT_hd3_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd4_UT_hd3_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.hd4_UT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd4->80*80, Upsample 4 times\n",
    "        self.hd5_UT_hd3 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd3_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd3_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, hd5_UT_hd3)\n",
    "        self.conv3d_1 = nn.Conv2d(384, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn3d_1 = SwitchNorm2d(self.UpChannels)\n",
    "        self.relu3d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        '''stage 2d '''\n",
    "        # self.att2 = attention.AttentionBlock(f_g=256, f_l=64, f_int=256)\n",
    "        self.att2 = ResidualBlockWithAttention(256,256)\n",
    "        \n",
    "        # h1->320*320, hd2->160*160, Pooling 2 times\n",
    "        self.h1_PT_hd2 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.h1_PT_hd2_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_PT_hd2_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.h1_PT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h2->160*160, hd2->160*160, Concatenation\n",
    "        self.h2_Cat_hd2_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "        self.h2_Cat_hd2_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.h2_Cat_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd3->80*80, hd2->160*160, Upsample 2 times\n",
    "        self.hd3_UT_hd2 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd3_UT_hd2_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd3_UT_hd2_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.hd3_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd4->40*40, hd2->160*160, Upsample 4 times\n",
    "        self.hd4_UT_hd2 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "        self.hd4_UT_hd2_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd4_UT_hd2_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.hd4_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd2->160*160, Upsample 8 times\n",
    "        self.hd5_UT_hd2 = nn.Upsample(scale_factor=8, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd2_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd2_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_PT_hd2, h2_Cat_hd2, hd3_UT_hd2, hd4_UT_hd2, hd5_UT_hd2)\n",
    "        self.conv2d_1 = nn.Conv2d(384, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn2d_1 = SwitchNorm2d(self.UpChannels)\n",
    "        self.relu2d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        '''stage 1d'''\n",
    "        # self.att1 = attention.AttentionBlock(f_g=256, f_l=64, f_int=256)\n",
    "        self.att1 = ResidualBlockWithAttention(256,256)\n",
    "\n",
    "        # h1->320*320, hd1->320*320, Concatenation\n",
    "        self.h1_Cat_hd1_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_Cat_hd1_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.h1_Cat_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd2->160*160, hd1->320*320, Upsample 2 times\n",
    "        self.hd2_UT_hd1 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd2_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd2_UT_hd1_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.hd2_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd3->80*80, hd1->320*320, Upsample 4 times\n",
    "        self.hd3_UT_hd1 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "        self.hd3_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd3_UT_hd1_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.hd3_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd4->40*40, hd1->320*320, Upsample 8 times\n",
    "        self.hd4_UT_hd1 = nn.Upsample(scale_factor=8, mode='bilinear')  # 14*14\n",
    "        self.hd4_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd4_UT_hd1_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.hd4_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd1->320*320, Upsample 16 times\n",
    "        self.hd5_UT_hd1 = nn.Upsample(scale_factor=16, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd1_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd1_bn = SwitchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_Cat_hd1, hd2_UT_hd1, hd3_UT_hd1, hd4_UT_hd1, hd5_UT_hd1)\n",
    "        self.conv1d_1 = nn.Conv2d(384, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn1d_1 = SwitchNorm2d(self.UpChannels)\n",
    "        self.relu1d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # output\n",
    "        self.outconv1 = nn.Conv2d(self.UpChannels, n_classes, 3, padding=1)\n",
    "\n",
    "        # initialise weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init_weights(m, init_type='kaiming')\n",
    "            elif isinstance(m, SwitchNorm2d):\n",
    "                init_weights(m, init_type='kaiming')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## -------------Encoder-------------\n",
    "        h1 = self.conv1(inputs)  # h1->320*320*64\n",
    "\n",
    "        h2 = self.maxpool1(h1)\n",
    "        h2 = self.conv2(h2)  # h2->160*160*128\n",
    "\n",
    "        h3 = self.maxpool2(h2)\n",
    "        h3 = self.conv3(h3)  # h3->80*80*256\n",
    "\n",
    "        h4 = self.maxpool3(h3)\n",
    "        h4 = self.conv4(h4)  # h4->40*40*512\n",
    "\n",
    "        h5 = self.maxpool4(h4)\n",
    "        hd5 = self.conv5(h5)  # h5->20*20*1024\n",
    "\n",
    "        ## -------------Decoder-------------\n",
    "        h1_PT_hd4 = self.h1_PT_hd4_relu(self.h1_PT_hd4_bn(self.h1_PT_hd4_conv(self.h1_PT_hd4(h1))))\n",
    "        h2_PT_hd4 = self.h2_PT_hd4_relu(self.h2_PT_hd4_bn(self.h2_PT_hd4_conv(self.h2_PT_hd4(h2))))\n",
    "        h3_PT_hd4 = self.h3_PT_hd4_relu(self.h3_PT_hd4_bn(self.h3_PT_hd4_conv(self.h3_PT_hd4(h3))))\n",
    "        h4_Cat_hd4 = self.h4_Cat_hd4_relu(self.h4_Cat_hd4_bn(self.h4_Cat_hd4_conv(h4)))\n",
    "        hd5_UT_hd4 = self.hd5_UT_hd4_relu(self.hd5_UT_hd4_bn(self.hd5_UT_hd4_conv(self.hd5_UT_hd4(hd5))))\n",
    "# #         modified\n",
    "        temp_cat4 = torch.cat((h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4), 1)\n",
    "        outatt4 = self.att4(g=temp_cat4, x=hd5_UT_hd4)\n",
    "#         print(\"outatt4\", outatt4.shape)\n",
    "#         print(\"hd5_UT_hd4\", hd5_UT_hd4.shape)\n",
    "        hd4 = self.relu4d_1(self.bn4d_1(self.conv4d_1(\n",
    "            torch.cat((h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4, outatt4, hd5_UT_hd4), 1)))) # hd4->40*40*UpChannels\n",
    "#         hd4 = self.relu4d_1(self.bn4d_1(self.conv4d_1(\n",
    "#             torch.cat((h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4, hd5_UT_hd4), 1)))) # hd4->40*40*UpChannels\n",
    "#         print(\"h4_Cat_hd4\", h4_Cat_hd4.shape)\n",
    "#         print(\"hd5_UT_hd4\", hd5_UT_hd4.shape)\n",
    "#         print(\"cat shape\", torch.cat((h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4, hd5_UT_hd4), 1).shape)\n",
    "#         print(\"hd4 shape\", hd4.shape)\n",
    "        \n",
    "# h4_Cat_hd4 torch.Size([26, 64, 16, 16])\n",
    "# hd5_UT_hd4 torch.Size([26, 64, 16, 16])\n",
    "# cat shape torch.Size([26, 320, 16, 16])\n",
    "# hd4 shape torch.Size([26, 320, 16, 16])\n",
    "\n",
    "        \n",
    "        h1_PT_hd3 = self.h1_PT_hd3_relu(self.h1_PT_hd3_bn(self.h1_PT_hd3_conv(self.h1_PT_hd3(h1))))\n",
    "        h2_PT_hd3 = self.h2_PT_hd3_relu(self.h2_PT_hd3_bn(self.h2_PT_hd3_conv(self.h2_PT_hd3(h2))))\n",
    "        h3_Cat_hd3 = self.h3_Cat_hd3_relu(self.h3_Cat_hd3_bn(self.h3_Cat_hd3_conv(h3)))\n",
    "        hd4_UT_hd3 = self.hd4_UT_hd3_relu(self.hd4_UT_hd3_bn(self.hd4_UT_hd3_conv(self.hd4_UT_hd3(hd4))))\n",
    "        hd5_UT_hd3 = self.hd5_UT_hd3_relu(self.hd5_UT_hd3_bn(self.hd5_UT_hd3_conv(self.hd5_UT_hd3(hd5))))\n",
    "        temp_cat3 = torch.cat((h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd5_UT_hd3), 1)\n",
    "        outatt3 = self.att3(g=temp_cat3, x=hd4_UT_hd3)\n",
    "#         print(\"outatt4\", outatt4.shape)\n",
    "#         print(\"hd5_UT_hd4\", hd5_UT_hd4.shape)\n",
    "        hd3 = self.relu3d_1(self.bn3d_1(self.conv3d_1(\n",
    "            torch.cat((h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, outatt3, hd5_UT_hd3), 1))))\n",
    "#         hd3 = self.relu3d_1(self.bn3d_1(self.conv3d_1(\n",
    "#             torch.cat((h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, hd5_UT_hd3), 1)))) # hd3->80*80*UpChannels\n",
    "\n",
    "        h1_PT_hd2 = self.h1_PT_hd2_relu(self.h1_PT_hd2_bn(self.h1_PT_hd2_conv(self.h1_PT_hd2(h1))))\n",
    "        h2_Cat_hd2 = self.h2_Cat_hd2_relu(self.h2_Cat_hd2_bn(self.h2_Cat_hd2_conv(h2)))\n",
    "        hd3_UT_hd2 = self.hd3_UT_hd2_relu(self.hd3_UT_hd2_bn(self.hd3_UT_hd2_conv(self.hd3_UT_hd2(hd3))))\n",
    "        hd4_UT_hd2 = self.hd4_UT_hd2_relu(self.hd4_UT_hd2_bn(self.hd4_UT_hd2_conv(self.hd4_UT_hd2(hd4))))\n",
    "        hd5_UT_hd2 = self.hd5_UT_hd2_relu(self.hd5_UT_hd2_bn(self.hd5_UT_hd2_conv(self.hd5_UT_hd2(hd5))))\n",
    "        temp_cat2 = torch.cat((h1_PT_hd2, h2_Cat_hd2, hd5_UT_hd2, hd4_UT_hd2), 1)\n",
    "        outatt2 = self.att2(g=temp_cat2, x=hd3_UT_hd2)\n",
    "#         hd2 = self.relu2d_1(self.bn2d_1(self.conv2d_1(\n",
    "#             torch.cat((h1_PT_hd2, h2_Cat_hd2, hd3_UT_hd2, hd4_UT_hd2, hd5_UT_hd2), 1)))) # hd2->160*160*UpChannels\n",
    "        hd2 = self.relu2d_1(self.bn2d_1(self.conv2d_1(torch.cat((h1_PT_hd2, h2_Cat_hd2, hd3_UT_hd2, hd4_UT_hd2, outatt2, hd5_UT_hd2), 1))))\n",
    "\n",
    "        h1_Cat_hd1 = self.h1_Cat_hd1_relu(self.h1_Cat_hd1_bn(self.h1_Cat_hd1_conv(h1)))\n",
    "        hd2_UT_hd1 = self.hd2_UT_hd1_relu(self.hd2_UT_hd1_bn(self.hd2_UT_hd1_conv(self.hd2_UT_hd1(hd2))))\n",
    "        hd3_UT_hd1 = self.hd3_UT_hd1_relu(self.hd3_UT_hd1_bn(self.hd3_UT_hd1_conv(self.hd3_UT_hd1(hd3))))\n",
    "        hd4_UT_hd1 = self.hd4_UT_hd1_relu(self.hd4_UT_hd1_bn(self.hd4_UT_hd1_conv(self.hd4_UT_hd1(hd4))))\n",
    "        hd5_UT_hd1 = self.hd5_UT_hd1_relu(self.hd5_UT_hd1_bn(self.hd5_UT_hd1_conv(self.hd5_UT_hd1(hd5))))\n",
    "#         hd1 = self.relu1d_1(self.bn1d_1(self.conv1d_1(\n",
    "#             torch.cat((h1_Cat_hd1, hd2_UT_hd1, hd3_UT_hd1, hd4_UT_hd1, hd5_UT_hd1), 1)))) # hd1->320*320*UpChannels\n",
    "        temp_cat1 = torch.cat((h1_Cat_hd1, hd5_UT_hd1, hd3_UT_hd1, hd4_UT_hd1), 1)\n",
    "        outatt1 = self.att1(g=temp_cat1, x=hd2_UT_hd1)\n",
    "        hd1 = self.relu1d_1(self.bn1d_1(self.conv1d_1(\n",
    "            torch.cat((h1_Cat_hd1, hd2_UT_hd1, hd3_UT_hd1, hd4_UT_hd1, outatt1, hd5_UT_hd1), 1))))\n",
    "        \n",
    "        d1 = self.outconv1(hd1)  # d1->320*320*n_classes\n",
    "        return F.sigmoid(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Residual Attention Mechanism Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Residual_Attention_Mechanism_Network\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import init\n",
    "# import functools\n",
    "# from torch.autograd import Variable\n",
    "# import numpy as np\n",
    "\n",
    "# from .basic_layers import ResidualBlock\n",
    "\n",
    "\n",
    "# class AttentionModule_pre(nn.Module):\n",
    "\n",
    "#     def __init__(self, in_channels, out_channels, size1, size2, size3):\n",
    "#         super(AttentionModule_pre, self).__init__()\n",
    "#         self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.trunk_branches = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#          )\n",
    "\n",
    "#         self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "#         self.softmax1_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "#         self.softmax2_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.skip2_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.mpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "#         self.softmax3_blocks = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#         )\n",
    "\n",
    "#         self.interpolation3 = nn.UpsamplingBilinear2d(size=size3)\n",
    "\n",
    "#         self.softmax4_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.interpolation2 = nn.UpsamplingBilinear2d(size=size2)\n",
    "\n",
    "#         self.softmax5_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n",
    "\n",
    "#         self.softmax6_blocks = nn.Sequential(\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels , kernel_size = 1, stride = 1, bias = False),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels , kernel_size = 1, stride = 1, bias = False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#         self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.first_residual_blocks(x)\n",
    "#         out_trunk = self.trunk_branches(x)\n",
    "#         out_mpool1 = self.mpool1(x)\n",
    "#         out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "#         out_skip1_connection = self.skip1_connection_residual_block(out_softmax1)\n",
    "#         out_mpool2 = self.mpool2(out_softmax1)\n",
    "#         out_softmax2 = self.softmax2_blocks(out_mpool2)\n",
    "#         out_skip2_connection = self.skip2_connection_residual_block(out_softmax2)\n",
    "#         out_mpool3 = self.mpool3(out_softmax2)\n",
    "#         out_softmax3 = self.softmax3_blocks(out_mpool3)\n",
    "#         #\n",
    "#         out_interp3 = self.interpolation3(out_softmax3)\n",
    "#         # print(out_skip2_connection.data)\n",
    "#         # print(out_interp3.data)\n",
    "#         out = out_interp3 + out_skip2_connection\n",
    "#         out_softmax4 = self.softmax4_blocks(out)\n",
    "#         out_interp2 = self.interpolation2(out_softmax4)\n",
    "#         out = out_interp2 + out_skip1_connection\n",
    "#         out_softmax5 = self.softmax5_blocks(out)\n",
    "#         out_interp1 = self.interpolation1(out_softmax5)\n",
    "#         out_softmax6 = self.softmax6_blocks(out_interp1)\n",
    "#         out = (1 + out_softmax6) * out_trunk\n",
    "#         out_last = self.last_blocks(out)\n",
    "\n",
    "#         return out_last\n",
    "\n",
    "\n",
    "# class AttentionModule_stage0(nn.Module):\n",
    "#     # input size is 112*112\n",
    "#     def __init__(self, in_channels, out_channels, size1=(112, 112), size2=(56, 56), size3=(28, 28), size4=(14, 14)):\n",
    "#         super(AttentionModule_stage0, self).__init__()\n",
    "#         self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.trunk_branches = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#          )\n",
    "\n",
    "#         self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "#         # 56*56\n",
    "#         self.softmax1_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "#         # 28*28\n",
    "#         self.softmax2_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.skip2_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.mpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "#         # 14*14\n",
    "#         self.softmax3_blocks = ResidualBlock(in_channels, out_channels)\n",
    "#         self.skip3_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "#         self.mpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "#         # 7*7\n",
    "#         self.softmax4_blocks = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#         )\n",
    "#         self.interpolation4 = nn.UpsamplingBilinear2d(size=size4)\n",
    "#         self.softmax5_blocks = ResidualBlock(in_channels, out_channels)\n",
    "#         self.interpolation3 = nn.UpsamplingBilinear2d(size=size3)\n",
    "#         self.softmax6_blocks = ResidualBlock(in_channels, out_channels)\n",
    "#         self.interpolation2 = nn.UpsamplingBilinear2d(size=size2)\n",
    "#         self.softmax7_blocks = ResidualBlock(in_channels, out_channels)\n",
    "#         self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n",
    "\n",
    "#         self.softmax8_blocks = nn.Sequential(\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias = False),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels , kernel_size=1, stride=1, bias = False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#         self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # 112*112\n",
    "#         x = self.first_residual_blocks(x)\n",
    "#         out_trunk = self.trunk_branches(x)\n",
    "#         out_mpool1 = self.mpool1(x)\n",
    "#         # 56*56\n",
    "#         out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "#         out_skip1_connection = self.skip1_connection_residual_block(out_softmax1)\n",
    "#         out_mpool2 = self.mpool2(out_softmax1)\n",
    "#         # 28*28\n",
    "#         out_softmax2 = self.softmax2_blocks(out_mpool2)\n",
    "#         out_skip2_connection = self.skip2_connection_residual_block(out_softmax2)\n",
    "#         out_mpool3 = self.mpool3(out_softmax2)\n",
    "#         # 14*14\n",
    "#         out_softmax3 = self.softmax3_blocks(out_mpool3)\n",
    "#         out_skip3_connection = self.skip3_connection_residual_block(out_softmax3)\n",
    "#         out_mpool4 = self.mpool4(out_softmax3)\n",
    "#         # 7*7\n",
    "#         out_softmax4 = self.softmax4_blocks(out_mpool4)\n",
    "#         out_interp4 = self.interpolation4(out_softmax4) + out_softmax3\n",
    "#         out = out_interp4 + out_skip3_connection\n",
    "#         out_softmax5 = self.softmax5_blocks(out)\n",
    "#         out_interp3 = self.interpolation3(out_softmax5) + out_softmax2\n",
    "#         # print(out_skip2_connection.data)\n",
    "#         # print(out_interp3.data)\n",
    "#         out = out_interp3 + out_skip2_connection\n",
    "#         out_softmax6 = self.softmax6_blocks(out)\n",
    "#         out_interp2 = self.interpolation2(out_softmax6) + out_softmax1\n",
    "#         out = out_interp2 + out_skip1_connection\n",
    "#         out_softmax7 = self.softmax7_blocks(out)\n",
    "#         out_interp1 = self.interpolation1(out_softmax7) + out_trunk\n",
    "#         out_softmax8 = self.softmax8_blocks(out_interp1)\n",
    "#         out = (1 + out_softmax8) * out_trunk\n",
    "#         out_last = self.last_blocks(out)\n",
    "\n",
    "#         return out_last\n",
    "\n",
    "\n",
    "# class AttentionModule_stage1(nn.Module):\n",
    "#     # input size is 56*56\n",
    "#     def __init__(self, in_channels, out_channels, size1=(56, 56), size2=(28, 28), size3=(14, 14)):\n",
    "#         super(AttentionModule_stage1, self).__init__()\n",
    "#         self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.trunk_branches = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#          )\n",
    "\n",
    "#         self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "#         self.softmax1_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "#         self.softmax2_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.skip2_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.mpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "#         self.softmax3_blocks = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#         )\n",
    "\n",
    "#         self.interpolation3 = nn.UpsamplingBilinear2d(size=size3)\n",
    "\n",
    "#         self.softmax4_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.interpolation2 = nn.UpsamplingBilinear2d(size=size2)\n",
    "\n",
    "#         self.softmax5_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n",
    "\n",
    "#         self.softmax6_blocks = nn.Sequential(\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels , kernel_size = 1, stride = 1, bias = False),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels , kernel_size = 1, stride = 1, bias = False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#         self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.first_residual_blocks(x)\n",
    "#         out_trunk = self.trunk_branches(x)\n",
    "#         out_mpool1 = self.mpool1(x)\n",
    "#         out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "#         out_skip1_connection = self.skip1_connection_residual_block(out_softmax1)\n",
    "#         out_mpool2 = self.mpool2(out_softmax1)\n",
    "#         out_softmax2 = self.softmax2_blocks(out_mpool2)\n",
    "#         out_skip2_connection = self.skip2_connection_residual_block(out_softmax2)\n",
    "#         out_mpool3 = self.mpool3(out_softmax2)\n",
    "#         out_softmax3 = self.softmax3_blocks(out_mpool3)\n",
    "#         #\n",
    "#         out_interp3 = self.interpolation3(out_softmax3) + out_softmax2\n",
    "#         # print(out_skip2_connection.data)\n",
    "#         # print(out_interp3.data)\n",
    "#         out = out_interp3 + out_skip2_connection\n",
    "#         out_softmax4 = self.softmax4_blocks(out)\n",
    "#         out_interp2 = self.interpolation2(out_softmax4) + out_softmax1\n",
    "#         out = out_interp2 + out_skip1_connection\n",
    "#         out_softmax5 = self.softmax5_blocks(out)\n",
    "#         out_interp1 = self.interpolation1(out_softmax5) + out_trunk\n",
    "#         out_softmax6 = self.softmax6_blocks(out_interp1)\n",
    "#         out = (1 + out_softmax6) * out_trunk\n",
    "#         out_last = self.last_blocks(out)\n",
    "\n",
    "#         return out_last\n",
    "\n",
    "\n",
    "# class AttentionModule_stage2(nn.Module):\n",
    "#     # input image size is 28*28\n",
    "#     def __init__(self, in_channels, out_channels, size1=(28, 28), size2=(14, 14)):\n",
    "#         super(AttentionModule_stage2, self).__init__()\n",
    "#         self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.trunk_branches = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#          )\n",
    "\n",
    "#         self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "#         self.softmax1_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "#         self.softmax2_blocks = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#         )\n",
    "\n",
    "#         self.interpolation2 = nn.UpsamplingBilinear2d(size=size2)\n",
    "\n",
    "#         self.softmax3_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n",
    "\n",
    "#         self.softmax4_blocks = nn.Sequential(\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#         self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.first_residual_blocks(x)\n",
    "#         out_trunk = self.trunk_branches(x)\n",
    "#         out_mpool1 = self.mpool1(x)\n",
    "#         out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "#         out_skip1_connection = self.skip1_connection_residual_block(out_softmax1)\n",
    "#         out_mpool2 = self.mpool2(out_softmax1)\n",
    "#         out_softmax2 = self.softmax2_blocks(out_mpool2)\n",
    "\n",
    "#         out_interp2 = self.interpolation2(out_softmax2) + out_softmax1\n",
    "#         # print(out_skip2_connection.data)\n",
    "#         # print(out_interp3.data)\n",
    "#         out = out_interp2 + out_skip1_connection\n",
    "#         out_softmax3 = self.softmax3_blocks(out)\n",
    "#         out_interp1 = self.interpolation1(out_softmax3) + out_trunk\n",
    "#         out_softmax4 = self.softmax4_blocks(out_interp1)\n",
    "#         out = (1 + out_softmax4) * out_trunk\n",
    "#         out_last = self.last_blocks(out)\n",
    "\n",
    "#         return out_last\n",
    "\n",
    "\n",
    "# class AttentionModule_stage3(nn.Module):\n",
    "#     # input image size is 14*14\n",
    "#     def __init__(self, in_channels, out_channels, size1=(14, 14)):\n",
    "#         super(AttentionModule_stage3, self).__init__()\n",
    "#         self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.trunk_branches = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#          )\n",
    "\n",
    "#         self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "#         self.softmax1_blocks = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#         )\n",
    "\n",
    "#         self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n",
    "\n",
    "#         self.softmax2_blocks = nn.Sequential(\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#         self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.first_residual_blocks(x)\n",
    "#         out_trunk = self.trunk_branches(x)\n",
    "#         out_mpool1 = self.mpool1(x)\n",
    "#         out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "\n",
    "#         out_interp1 = self.interpolation1(out_softmax1) + out_trunk\n",
    "#         out_softmax2 = self.softmax2_blocks(out_interp1)\n",
    "#         out = (1 + out_softmax2) * out_trunk\n",
    "#         out_last = self.last_blocks(out)\n",
    "\n",
    "#         return out_last\n",
    "\n",
    "\n",
    "# class AttentionModule_stage1_cifar(nn.Module):\n",
    "#     # input size is 16*16\n",
    "#     def __init__(self, in_channels, out_channels, size1=(16, 16), size2=(8, 8)):\n",
    "#         super(AttentionModule_stage1_cifar, self).__init__()\n",
    "#         self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.trunk_branches = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#          )\n",
    "\n",
    "#         self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 8*8\n",
    "\n",
    "#         self.down_residual_blocks1 = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 4*4\n",
    "\n",
    "#         self.middle_2r_blocks = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#         )\n",
    "\n",
    "#         self.interpolation1 = nn.UpsamplingBilinear2d(size=size2)  # 8*8\n",
    "\n",
    "#         self.up_residual_blocks1 = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.interpolation2 = nn.UpsamplingBilinear2d(size=size1)  # 16*16\n",
    "\n",
    "#         self.conv1_1_blocks = nn.Sequential(\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias = False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#         self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.first_residual_blocks(x)\n",
    "#         out_trunk = self.trunk_branches(x)\n",
    "#         out_mpool1 = self.mpool1(x)\n",
    "#         out_down_residual_blocks1 = self.down_residual_blocks1(out_mpool1)\n",
    "#         out_skip1_connection = self.skip1_connection_residual_block(out_down_residual_blocks1)\n",
    "#         out_mpool2 = self.mpool2(out_down_residual_blocks1)\n",
    "#         out_middle_2r_blocks = self.middle_2r_blocks(out_mpool2)\n",
    "#         #\n",
    "#         out_interp = self.interpolation1(out_middle_2r_blocks) + out_down_residual_blocks1\n",
    "#         # print(out_skip2_connection.data)\n",
    "#         # print(out_interp3.data)\n",
    "#         out = out_interp + out_skip1_connection\n",
    "#         out_up_residual_blocks1 = self.up_residual_blocks1(out)\n",
    "#         out_interp2 = self.interpolation2(out_up_residual_blocks1) + out_trunk\n",
    "#         out_conv1_1_blocks = self.conv1_1_blocks(out_interp2)\n",
    "#         out = (1 + out_conv1_1_blocks) * out_trunk\n",
    "#         out_last = self.last_blocks(out)\n",
    "\n",
    "#         return out_last\n",
    "\n",
    "\n",
    "# class AttentionModule_stage2_cifar(nn.Module):\n",
    "#     # input size is 8*8\n",
    "#     def __init__(self, in_channels, out_channels, size=(8, 8)):\n",
    "#         super(AttentionModule_stage2_cifar, self).__init__()\n",
    "#         self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.trunk_branches = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#          )\n",
    "\n",
    "#         self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 4*4\n",
    "\n",
    "#         self.middle_2r_blocks = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#         )\n",
    "\n",
    "#         self.interpolation1 = nn.UpsamplingBilinear2d(size=size)  # 8*8\n",
    "\n",
    "#         self.conv1_1_blocks = nn.Sequential(\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias = False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#         self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.first_residual_blocks(x)\n",
    "#         out_trunk = self.trunk_branches(x)\n",
    "#         out_mpool1 = self.mpool1(x)\n",
    "#         out_middle_2r_blocks = self.middle_2r_blocks(out_mpool1)\n",
    "#         #\n",
    "#         out_interp = self.interpolation1(out_middle_2r_blocks) + out_trunk\n",
    "#         # print(out_skip2_connection.data)\n",
    "#         # print(out_interp3.data)\n",
    "#         out_conv1_1_blocks = self.conv1_1_blocks(out_interp)\n",
    "#         out = (1 + out_conv1_1_blocks) * out_trunk\n",
    "#         out_last = self.last_blocks(out)\n",
    "\n",
    "#         return out_last\n",
    "\n",
    "\n",
    "# class AttentionModule_stage3_cifar(nn.Module):\n",
    "#     # input size is 4*4\n",
    "#     def __init__(self, in_channels, out_channels, size=(8, 8)):\n",
    "#         super(AttentionModule_stage3_cifar, self).__init__()\n",
    "#         self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#         self.trunk_branches = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#          )\n",
    "\n",
    "#         self.middle_2r_blocks = nn.Sequential(\n",
    "#             ResidualBlock(in_channels, out_channels),\n",
    "#             ResidualBlock(in_channels, out_channels)\n",
    "#         )\n",
    "\n",
    "#         self.conv1_1_blocks = nn.Sequential(\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias = False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#         self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.first_residual_blocks(x)\n",
    "#         out_trunk = self.trunk_branches(x)\n",
    "#         out_middle_2r_blocks = self.middle_2r_blocks(x)\n",
    "#         #\n",
    "#         out_conv1_1_blocks = self.conv1_1_blocks(out_middle_2r_blocks)\n",
    "#         out = (1 + out_conv1_1_blocks) * out_trunk\n",
    "#         out_last = self.last_blocks(out)\n",
    "\n",
    "#         return out_last\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Switch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Switch_Normalization\n",
    "\n",
    "#Switchable Normalization is a normalization technique that is able to learn different normalization operations \n",
    "#for different normalization layers in a deep neural network in an end-to-end manner.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SwitchNorm1d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.997, using_moving_average=True):\n",
    "        super(SwitchNorm1d, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.using_moving_average = using_moving_average\n",
    "        self.weight = nn.Parameter(torch.ones(1, num_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(1, num_features))\n",
    "        self.mean_weight = nn.Parameter(torch.ones(2))\n",
    "        self.var_weight = nn.Parameter(torch.ones(2))\n",
    "        self.register_buffer('running_mean', torch.zeros(1, num_features))\n",
    "        self.register_buffer('running_var', torch.zeros(1, num_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.running_mean.zero_()\n",
    "        self.running_var.zero_()\n",
    "        self.weight.data.fill_(1)\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 2:\n",
    "            raise ValueError('expected 2D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._check_input_dim(x)\n",
    "        mean_ln = x.mean(1, keepdim=True)\n",
    "        var_ln = x.var(1, keepdim=True)\n",
    "\n",
    "        if self.training:\n",
    "            mean_bn = x.mean(0, keepdim=True)\n",
    "            var_bn = x.var(0, keepdim=True)\n",
    "            if self.using_moving_average:\n",
    "                self.running_mean.mul_(self.momentum)\n",
    "                self.running_mean.add_((1 - self.momentum) * mean_bn.data)\n",
    "                self.running_var.mul_(self.momentum)\n",
    "                self.running_var.add_((1 - self.momentum) * var_bn.data)\n",
    "            else:\n",
    "                self.running_mean.add_(mean_bn.data)\n",
    "                self.running_var.add_(mean_bn.data ** 2 + var_bn.data)\n",
    "        else:\n",
    "            mean_bn = torch.autograd.Variable(self.running_mean)\n",
    "            var_bn = torch.autograd.Variable(self.running_var)\n",
    "\n",
    "        softmax = nn.Softmax(0)\n",
    "        mean_weight = softmax(self.mean_weight)\n",
    "        var_weight = softmax(self.var_weight)\n",
    "\n",
    "        mean = mean_weight[0] * mean_ln + mean_weight[1] * mean_bn\n",
    "        var = var_weight[0] * var_ln + var_weight[1] * var_bn\n",
    "\n",
    "        x = (x - mean) / (var + self.eps).sqrt()\n",
    "        return x * self.weight + self.bias\n",
    "\n",
    "class SwitchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.9, using_moving_average=True, using_bn=True,\n",
    "                 last_gamma=False):\n",
    "        super(SwitchNorm2d, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.using_moving_average = using_moving_average\n",
    "        self.using_bn = using_bn\n",
    "        self.last_gamma = last_gamma\n",
    "        self.weight = nn.Parameter(torch.ones(1, num_features, 1, 1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1, num_features, 1, 1))\n",
    "        if self.using_bn:\n",
    "            self.mean_weight = nn.Parameter(torch.ones(3))\n",
    "            self.var_weight = nn.Parameter(torch.ones(3))\n",
    "        else:\n",
    "            self.mean_weight = nn.Parameter(torch.ones(2))\n",
    "            self.var_weight = nn.Parameter(torch.ones(2))\n",
    "        if self.using_bn:\n",
    "            self.register_buffer('running_mean', torch.zeros(1, num_features, 1))\n",
    "            self.register_buffer('running_var', torch.zeros(1, num_features, 1))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.using_bn:\n",
    "            self.running_mean.zero_()\n",
    "            self.running_var.zero_()\n",
    "        if self.last_gamma:\n",
    "            self.weight.data.fill_(0)\n",
    "        else:\n",
    "            self.weight.data.fill_(1)\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 4:\n",
    "            raise ValueError('expected 4D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._check_input_dim(x)\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.view(N, C, -1)\n",
    "        mean_in = x.mean(-1, keepdim=True)\n",
    "        var_in = x.var(-1, keepdim=True)\n",
    "\n",
    "        mean_ln = mean_in.mean(1, keepdim=True)\n",
    "        temp = var_in + mean_in ** 2\n",
    "        var_ln = temp.mean(1, keepdim=True) - mean_ln ** 2\n",
    "\n",
    "        if self.using_bn:\n",
    "            if self.training:\n",
    "                mean_bn = mean_in.mean(0, keepdim=True)\n",
    "                var_bn = temp.mean(0, keepdim=True) - mean_bn ** 2\n",
    "                if self.using_moving_average:\n",
    "                    self.running_mean.mul_(self.momentum)\n",
    "                    self.running_mean.add_((1 - self.momentum) * mean_bn.data)\n",
    "                    self.running_var.mul_(self.momentum)\n",
    "                    self.running_var.add_((1 - self.momentum) * var_bn.data)\n",
    "                else:\n",
    "                    self.running_mean.add_(mean_bn.data)\n",
    "                    self.running_var.add_(mean_bn.data ** 2 + var_bn.data)\n",
    "            else:\n",
    "                mean_bn = torch.autograd.Variable(self.running_mean)\n",
    "                var_bn = torch.autograd.Variable(self.running_var)\n",
    "\n",
    "        softmax = nn.Softmax(0)\n",
    "        mean_weight = softmax(self.mean_weight)\n",
    "        var_weight = softmax(self.var_weight)\n",
    "\n",
    "        if self.using_bn:\n",
    "            mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln + mean_weight[2] * mean_bn\n",
    "            var = var_weight[0] * var_in + var_weight[1] * var_ln + var_weight[2] * var_bn\n",
    "        else:\n",
    "            mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln\n",
    "            var = var_weight[0] * var_in + var_weight[1] * var_ln\n",
    "\n",
    "        x = (x-mean) / (var+self.eps).sqrt()\n",
    "        x = x.view(N, C, H, W)\n",
    "        return x * self.weight + self.bias\n",
    "\n",
    "\n",
    "class SwitchNorm3d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.997, using_moving_average=True, using_bn=True,\n",
    "                 last_gamma=False):\n",
    "        super(SwitchNorm3d, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.using_moving_average = using_moving_average\n",
    "        self.using_bn = using_bn\n",
    "        self.last_gamma = last_gamma\n",
    "        self.weight = nn.Parameter(torch.ones(1, num_features, 1, 1, 1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1, num_features, 1, 1, 1))\n",
    "        if self.using_bn:\n",
    "            self.mean_weight = nn.Parameter(torch.ones(3))\n",
    "            self.var_weight = nn.Parameter(torch.ones(3))\n",
    "        else:\n",
    "            self.mean_weight = nn.Parameter(torch.ones(2))\n",
    "            self.var_weight = nn.Parameter(torch.ones(2))\n",
    "        if self.using_bn:\n",
    "            self.register_buffer('running_mean', torch.zeros(1, num_features, 1))\n",
    "            self.register_buffer('running_var', torch.zeros(1, num_features, 1))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.using_bn:\n",
    "            self.running_mean.zero_()\n",
    "            self.running_var.zero_()\n",
    "        if self.last_gamma:\n",
    "            self.weight.data.fill_(0)\n",
    "        else:\n",
    "            self.weight.data.fill_(1)\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 5:\n",
    "            raise ValueError('expected 5D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._check_input_dim(x)\n",
    "        N, C, D, H, W = x.size()\n",
    "        x = x.view(N, C, -1)\n",
    "        mean_in = x.mean(-1, keepdim=True)\n",
    "        var_in = x.var(-1, keepdim=True)\n",
    "\n",
    "        mean_ln = mean_in.mean(1, keepdim=True)\n",
    "        temp = var_in + mean_in ** 2\n",
    "        var_ln = temp.mean(1, keepdim=True) - mean_ln ** 2\n",
    "\n",
    "        if self.using_bn:\n",
    "            if self.training:\n",
    "                mean_bn = mean_in.mean(0, keepdim=True)\n",
    "                var_bn = temp.mean(0, keepdim=True) - mean_bn ** 2\n",
    "                if self.using_moving_average:\n",
    "                    self.running_mean.mul_(self.momentum)\n",
    "                    self.running_mean.add_((1 - self.momentum) * mean_bn.data)\n",
    "                    self.running_var.mul_(self.momentum)\n",
    "                    self.running_var.add_((1 - self.momentum) * var_bn.data)\n",
    "                else:\n",
    "                    self.running_mean.add_(mean_bn.data)\n",
    "                    self.running_var.add_(mean_bn.data ** 2 + var_bn.data)\n",
    "            else:\n",
    "                mean_bn = torch.autograd.Variable(self.running_mean)\n",
    "                var_bn = torch.autograd.Variable(self.running_var)\n",
    "\n",
    "        softmax = nn.Softmax(0)\n",
    "        mean_weight = softmax(self.mean_weight)\n",
    "        var_weight = softmax(self.var_weight)\n",
    "\n",
    "        if self.using_bn:\n",
    "            mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln + mean_weight[2] * mean_bn\n",
    "            var = var_weight[0] * var_in + var_weight[1] * var_ln + var_weight[2] * var_bn\n",
    "        else:\n",
    "            mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln\n",
    "            var = var_weight[0] * var_in + var_weight[1] * var_ln\n",
    "\n",
    "        x = (x - mean) / (var + self.eps).sqrt()\n",
    "        x = x.view(N, C, D, H, W)\n",
    "        return x * self.weight + self.bias\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Metrics\n",
    "#Dice_Metrics\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def dice_coef_metric(inputs, target): # ORIGINAL\n",
    "    intersection = 2.0 * (target*inputs).sum()\n",
    "    union = target.sum() + inputs.sum()\n",
    "    if target.sum() == 0 and inputs.sum() == 0:\n",
    "        return 1.0 \n",
    "    return intersection/union\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "#         inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "def compute_iou(model, loader, device:str, threshold=0.3):\n",
    "    valloss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i_step, (data, target) in enumerate(loader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "\n",
    "            out_cut = np.copy(outputs.data.cpu().numpy())\n",
    "            out_cut[np.nonzero(out_cut < threshold)] = 0.0\n",
    "            out_cut[np.nonzero(out_cut >= threshold)] = 1.0\n",
    "            picloss = dice_coef_metric(out_cut, target.data.cpu().numpy())\n",
    "            valloss += picloss\n",
    "\n",
    "    return valloss / i_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_PATH= \"C:\\\\Users\\\\jervi\\\\Downloads\\\\kaggle_3m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location= \"C:\\\\Users\\\\jervi\\\\Downloads\\\\kaggle_3m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket = \"heartnet-sagemaker-project/dataset/1/\"\n",
    "# data_key = \"train.csv\"\n",
    "# data_location = \"s3://{}/{}\".format(bucket,data_key)\n",
    "\n",
    "# pd.read_csv(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket = \"heartnet-sagemaker-project/kaggle_3m/\"\n",
    "# data_key = \"train.csv\"\n",
    "# data_location = \"s3://{}/{}\".format(bucket,data_key)\n",
    "\n",
    "# pd.read_csv(data_location)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LEN = len(data_location) + len(\"\\\\TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_\")\n",
    "END_LEN = len(\".tif\")\n",
    "END_MASK_LEN = len(\"_mask.tif\")\n",
    "\n",
    "IMG_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_LEN = len(BASE_PATH) + len(\"\\\\TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_\")\n",
    "# END_LEN = len(\".tif\")\n",
    "# END_MASK_LEN = len(\"_mask.tif\")\n",
    "\n",
    "# IMG_SIZE = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] This is not a dir --> C:\\Users\\jervi\\Downloads\\kaggle_3m\\data.csv\n",
      "[INFO] This is not a dir --> C:\\Users\\jervi\\Downloads\\kaggle_3m\\README.md\n"
     ]
    }
   ],
   "source": [
    "df = get_dataset_dataframe(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_dataset_dataframe(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dir_name</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA_CS_4942_19970222</td>\n",
       "      <td>C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_CS_494...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA_CS_4942_19970222</td>\n",
       "      <td>C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_CS_494...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA_CS_4942_19970222</td>\n",
       "      <td>C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_CS_494...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA_CS_4942_19970222</td>\n",
       "      <td>C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_CS_494...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA_CS_4942_19970222</td>\n",
       "      <td>C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_CS_494...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>TCGA_HT_A61A_20000127</td>\n",
       "      <td>C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_HT_A61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>TCGA_HT_A61A_20000127</td>\n",
       "      <td>C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_HT_A61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>TCGA_HT_A61A_20000127</td>\n",
       "      <td>C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_HT_A61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>TCGA_HT_A61A_20000127</td>\n",
       "      <td>C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_HT_A61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>TCGA_HT_A61A_20000127</td>\n",
       "      <td>C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_HT_A61...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3018 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dir_name                                         image_path\n",
       "0     TCGA_CS_4942_19970222  C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_CS_494...\n",
       "1     TCGA_CS_4942_19970222  C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_CS_494...\n",
       "2     TCGA_CS_4942_19970222  C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_CS_494...\n",
       "3     TCGA_CS_4942_19970222  C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_CS_494...\n",
       "4     TCGA_CS_4942_19970222  C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_CS_494...\n",
       "...                     ...                                                ...\n",
       "3013  TCGA_HT_A61A_20000127  C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_HT_A61...\n",
       "3014  TCGA_HT_A61A_20000127  C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_HT_A61...\n",
       "3015  TCGA_HT_A61A_20000127  C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_HT_A61...\n",
       "3016  TCGA_HT_A61A_20000127  C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_HT_A61...\n",
       "3017  TCGA_HT_A61A_20000127  C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_HT_A61...\n",
       "\n",
       "[3018 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dir_name      0\n",
       "image_path    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs = df[~df[\"image_path\"].str.contains(\"mask\")]\n",
    "df_masks = df[df[\"image_path\"].str.contains(\"mask\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imgs.iloc[0,1][BASE_LEN: -END_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = sorted(df_imgs[\"image_path\"].values, key= lambda x: int((x[BASE_LEN: -END_LEN])))\n",
    "masks = sorted(df_masks[\"image_path\"].values, key=lambda x: int((x[BASE_LEN: -END_MASK_LEN])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1526, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image *C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_FG_7634_20000128\\TCGA_FG_7634_20000128_12.tif*\n",
      " Belongs to the mask *C:\\Users\\jervi\\Downloads\\kaggle_3m\\TCGA_DU_8162_19961029\\TCGA_DU_8162_19961029_12_mask.tif*\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "idx = random.randint(0, len(imgs)-1)\n",
    "print(f\"This image *{imgs[idx]}*\\n Belongs to the mask *{masks[idx]}*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dir_name', 'image_path'], dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imgs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       TCGA_CS_4942_19970222\n",
       "1       TCGA_CS_4942_19970222\n",
       "3       TCGA_CS_4942_19970222\n",
       "5       TCGA_CS_4942_19970222\n",
       "7       TCGA_CS_4942_19970222\n",
       "                ...          \n",
       "3006    TCGA_HT_A61A_20000127\n",
       "3008    TCGA_HT_A61A_20000127\n",
       "3009    TCGA_HT_A61A_20000127\n",
       "3011    TCGA_HT_A61A_20000127\n",
       "3016    TCGA_HT_A61A_20000127\n",
       "Name: dir_name, Length: 1492, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imgs.dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1492, 1526)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgs),len(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# final dataframe\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dff \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m\"\u001b[39;49m\u001b[39mpatient\u001b[39;49m\u001b[39m\"\u001b[39;49m: df_imgs,\n\u001b[0;32m      3\u001b[0m                    \u001b[39m\"\u001b[39;49m\u001b[39mimage_path\u001b[39;49m\u001b[39m\"\u001b[39;49m: imgs,\n\u001b[0;32m      4\u001b[0m                    \u001b[39m\"\u001b[39;49m\u001b[39mmask_path\u001b[39;49m\u001b[39m\"\u001b[39;49m: masks})\n\u001b[0;32m      6\u001b[0m dff\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\jervi\\anaconda3\\envs\\PhaseOne\\lib\\site-packages\\pandas\\core\\frame.py:662\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    656\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    657\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    658\u001b[0m     )\n\u001b[0;32m    660\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    661\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 662\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    663\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    664\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jervi\\anaconda3\\envs\\PhaseOne\\lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\Users\\jervi\\anaconda3\\envs\\PhaseOne\\lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    116\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    119\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\jervi\\anaconda3\\envs\\PhaseOne\\lib\\site-packages\\pandas\\core\\internals\\construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    664\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[0;32m    665\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 666\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    668\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[0;32m    669\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    670\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    671\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# final dataframe\n",
    "dff = pd.DataFrame({\"patient\": df_imgs.dir_name.values,\n",
    "                   \"image_path\": imgs,\n",
    "                   \"mask_path\": masks})\n",
    "\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.iloc[0,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff[\"diagnosis\"] = dff[\"mask_path\"].apply(lambda x: pos_neg_diagnosis(x))\n",
    "\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amount of patients: \", len(set(dff.patient)))\n",
    "print(\"Amount of records: \", len(dff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import sklearn as sk\n",
    "import sys\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "print(f\"Target device is {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(dff, stratify=dff.diagnosis, test_size=0.1)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "train_df, test_df = train_test_split(train_df, stratify=train_df.diagnosis, test_size=0.12)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {train_df.shape} \\nVal: {val_df.shape} \\nTest: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MRImagingDataset(train_df, transform=augmentData.transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "val_dataset = MRImagingDataset(val_df, transform=augmentData.transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=10,  shuffle=True)\n",
    "\n",
    "test_dataset = MRImagingDataset(test_df, transform=augmentData.transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images, masks = next(iter(train_dataloader))\n",
    "print(images.shape, masks.shape)\n",
    "\n",
    "show_aug(images)\n",
    "show_aug(masks, norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sanity\n",
    "output = torch.randn(1,3,256,256).to(device)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "DiceLoss()(F.sigmoid(torch.tensor([0.7, 1., 1.])), \n",
    "              torch.tensor([1.,1.,1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net 3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet3p = UNet3Plus(n_classes=1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = \"model_unet_3p.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_unet_3p = torch.optim.Adamax(unet3p.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_unet_3p = torch.optim.ASGD(unet3p.parameters(), lr=1e-3, lambd= 0.04, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "num_ep = 30\n",
    "# try until 30\n",
    "\n",
    "# aun_lh, aun_th, aun_vh = train_model(\"Attention UNet\", attention_unet, train_dataloader, val_dataloader, DiceLoss(), opt, False, num_ep)\n",
    "aun_lh, aun_th, aun_vh = train_model(\"UNet3p\", unet3p, train_dataloader, val_dataloader, DiceLoss(), opt_unet_3p, False, num_ep, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(\"U-Net 3+\", aun_th, aun_vh, num_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_ep), aun_lh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iou = compute_iou(unet3p, test_dataloader,device)\n",
    "print(f\"\"\"U-Net 3+\\nMean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.df[test_dataset.df.diagnosis==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[1][1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1669\n",
    "plt.imshow(test_dataset.get_image_and_mask(idx)[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ, op = viz_pred_output(model,test_dataloader, idx, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{np.around(dice_coef_metric(op, targ), 2)*100}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2032\n",
    "plt.imshow(test_dataset.get_image_and_mask(idx)[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ, op = viz_pred_output(model,test_dataloader, idx, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{np.around(dice_coef_metric(op, targ), 2)*100}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1819\n",
    "plt.imshow(test_dataset.get_image_and_mask(idx)[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ, op = viz_pred_output(model,test_dataloader, idx, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{np.around(dice_coef_metric(op, targ), 2)*100}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =691\n",
    "plt.imshow(test_dataset.get_image_and_mask(idx)[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ, op = viz_pred_output(model,test_dataloader, idx, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{np.around(dice_coef_metric(op, targ), 2)*100}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =2243\n",
    "plt.imshow(test_dataset.get_image_and_mask(idx)[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ, op = viz_pred_output(model,test_dataloader, idx, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{np.around(dice_coef_metric(op, targ), 2)*100}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =737\n",
    "plt.imshow(test_dataset.get_image_and_mask(idx)[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "targ, op = viz_pred_output(model,test_dataloader, idx, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{np.around(dice_coef_metric(op, targ), 2)*100}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =2194\n",
    "plt.imshow(test_dataset.get_image_and_mask(idx)[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ, op = viz_pred_output(model,test_dataloader, idx, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{np.around(dice_coef_metric(op, targ), 2)*100}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =459\n",
    "plt.imshow(test_dataset.get_image_and_mask(idx)[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ, op = viz_pred_output(model,test_dataloader, idx, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{np.around(dice_coef_metric(op, targ), 2)*100}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =1440\n",
    "plt.imshow(test_dataset.get_image_and_mask(idx)[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ, op = viz_pred_output(model,test_dataloader, idx, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{np.around(dice_coef_metric(op, targ), 2)*100}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =2616\n",
    "plt.imshow(test_dataset.get_image_and_mask(idx)[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ, op = viz_pred_output(model,test_dataloader, idx, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{np.around(dice_coef_metric(op, targ), 2)*100}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-NET 3+ with ResidualBlock With Attention (UNet_3Plus_ResAttn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unet_3plus_resattn = UNet_3Plus_ResAttn(n_classes=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# PATH = \"model_unet_3p_attn.pt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_unet_3plus_resattn = torch.optim.Adamax(unet_3plus_resattn.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckp_path = \"path/to/checkpoint/checkpoint.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "num_ep = 3\n",
    "# try until 30\n",
    "\n",
    "# aun_lh, aun_th, aun_vh = train_model(\"Attention UNet\", attention_unet, train_dataloader, val_dataloader, DiceLoss(), opt, False, num_ep)\n",
    "aun_lh, aun_th, aun_vh = train_model(\"UNet_3p_attn\", unet3p_attn, train_dataloader, val_dataloader, DiceLoss(), opt_unet_3plus_resattn, False, num_ep, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(\" U-Net 3+ ResAttn\", aun_th, aun_vh, num_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_ep), aun_lh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iou = compute_iou(unet_3plus_resattn, test_dataloader, device)\n",
    "print(f\"\"\"U-Net 3+ Residual Attention\\nMean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aun_lh_prev, aun_th_prev, aun_vh_prev = aun_lh, aun_th, aun_vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_num_ep = num_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata():\n",
    "    #Saving initial history\n",
    "    np.savetxt('metadata/unet_3plus_resattn/aun_lh_prev.txt', aun_lh_prev)\n",
    "    np.savetxt('metadata/unet_3plus_resattn/aun_th_prev.txt', aun_th_prev)\n",
    "    np.savetxt('metadata/unet_3plus_resattn/aun_vh_prev.txt', aun_vh_prev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTINUE WHERE LEFT OFF / TRAIN MORE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aun_lh_prev = np.loadtxt('metadata/unet_3plus_resattn/aun_lh_prev.txt')\n",
    "aun_th_prev = np.loadtxt('metadata/unet_3plus_resattn/aun_th_prev.txt')\n",
    "aun_vh_prev = np.loadtxt('metadata/unet_3plus_resattn/aun_vh_prev.txt')\n",
    "last_num_ep = len(aun_lh_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_x_more_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(\"U-Net 3+ with Attention Gates\", aun_th_prev, aun_vh_prev, last_num_ep)\n",
    "# plt.savefig(f'UNet_3p_attn_dice.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(last_num_ep), aun_lh_prev)\n",
    "plt.savefig(f'UNet_3p_attn_loss.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iou = compute_iou(unet3p_attn, test_dataloader, device)\n",
    "print(f\"\"\"U-Net 3+ Attention\\nMean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhaseOne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
